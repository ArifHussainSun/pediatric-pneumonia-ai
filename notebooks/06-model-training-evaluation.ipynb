{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation Framework\n",
    "\n",
    "This notebook provides a comprehensive framework for training and evaluating all pneumonia detection models developed in this project. It includes:\n",
    "\n",
    "1. **Unified Training Pipeline**: Standardized training procedures for all model types\n",
    "2. **Evaluation Metrics**: Comprehensive performance assessment\n",
    "3. **Model Comparison**: Side-by-side analysis of different approaches\n",
    "4. **Hyperparameter Optimization**: Systematic parameter tuning\n",
    "5. **Cross-Validation**: Robust performance estimation\n",
    "6. **Model Persistence**: Saving and loading trained models\n",
    "\n",
    "This framework enables reproducible training and fair comparison of all pneumonia detection approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "MODELS_DIR = Path(\"../models\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PneumoniaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for pneumonia detection with flexible transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform=None, split='train'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_idx, class_name in enumerate(['normal', 'pneumonia']):\n",
    "            class_dir = self.data_dir / split / class_name\n",
    "            if class_dir.exists():\n",
    "                for img_path in class_dir.glob('*.jpg'):\n",
    "                    self.samples.append(str(img_path))\n",
    "                    self.labels.append(class_idx)\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} {split} images\")\n",
    "        print(f\"Class distribution: Normal={self.labels.count(0)}, Pneumonia={self.labels.count(1)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def get_data_transforms(image_size=224, augment=True):\n",
    "    \"\"\"\n",
    "    Get data transformation pipelines for training and validation\n",
    "    \"\"\"\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def create_data_loaders(data_dir, batch_size=32, image_size=224, val_split=0.2):\n",
    "    \"\"\"\n",
    "    Create train and validation data loaders\n",
    "    \"\"\"\n",
    "    train_transform, val_transform = get_data_transforms(image_size)\n",
    "    \n",
    "    # Load full training dataset\n",
    "    full_dataset = PneumoniaDataset(data_dir, train_transform, 'train')\n",
    "    \n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_dataset) * val_split)\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Update validation dataset transform\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Test loader\n",
    "    test_dataset = PneumoniaDataset(data_dir, val_transform, 'test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionModel(nn.Module):\n",
    "    \"\"\"Xception model for pneumonia classification\"\"\"\n",
    "    def __init__(self, pretrained=True, freeze_layers=100):\n",
    "        super(XceptionModel, self).__init__()\n",
    "        self.xception = timm.create_model('xception', pretrained=pretrained)\n",
    "        \n",
    "        # Freeze early layers\n",
    "        for i, (name, param) in enumerate(self.xception.named_parameters()):\n",
    "            if i < freeze_layers:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace classifier\n",
    "        self.xception.global_pool = nn.Identity()\n",
    "        self.xception.fc = nn.Identity()\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.xception(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class XceptionLSTM(nn.Module):\n",
    "    \"\"\"Xception-LSTM hybrid model\"\"\"\n",
    "    def __init__(self, pretrained=True, freeze_layers=100):\n",
    "        super(XceptionLSTM, self).__init__()\n",
    "        self.xception = timm.create_model(\"xception\", pretrained=pretrained, features_only=True)\n",
    "        \n",
    "        # Freeze early layers\n",
    "        for i, (name, param) in enumerate(self.xception.named_parameters()):\n",
    "            if i < freeze_layers:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.reshape = nn.Flatten(2)\n",
    "        self.transpose = lambda x: x.permute(0, 2, 1)\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=256, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.dropout = nn.Dropout(0.46)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.xception(x)[-1]\n",
    "        x = self.pool(x)\n",
    "        x = self.reshape(x)\n",
    "        x = self.transpose(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    \"\"\"Fusion model combining Xception and VGG16\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(FusionModel, self).__init__()\n",
    "        \n",
    "        # Xception branch\n",
    "        self.xception = timm.create_model('xception', pretrained=pretrained)\n",
    "        self.xception.global_pool = nn.Identity()\n",
    "        self.xception.fc = nn.Identity()\n",
    "        \n",
    "        # VGG16 branch\n",
    "        self.vgg = torchvision.models.vgg16(pretrained=pretrained)\n",
    "        self.vgg.classifier = nn.Identity()\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.xception_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.vgg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fusion classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2048 + 512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Xception path\n",
    "        x1 = self.xception(x)\n",
    "        x1 = self.xception_pool(x1).view(x.size(0), -1)\n",
    "        \n",
    "        # VGG16 path\n",
    "        x2 = self.vgg.features(x)\n",
    "        x2 = self.vgg_pool(x2).view(x.size(0), -1)\n",
    "        \n",
    "        # Concatenate and classify\n",
    "        fused = torch.cat((x1, x2), dim=1)\n",
    "        out = self.classifier(fused)\n",
    "        return out\n",
    "\n",
    "def get_model(model_name, **kwargs):\n",
    "    \"\"\"Factory function to create models\"\"\"\n",
    "    models = {\n",
    "        'xception': XceptionModel,\n",
    "        'xception_lstm': XceptionLSTM,\n",
    "        'fusion': FusionModel\n",
    "    }\n",
    "    \n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return models[model_name](**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Unified training framework for all model types\"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, optimizer, device=None, scheduler=None):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device or DEVICE\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'lr': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc='Training')\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(self.device), labels.float().to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs).squeeze()\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(progress_bar.n+1):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate_epoch(self, val_loader):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.float().to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs).squeeze()\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs, early_stopping_patience=None):\n",
    "        \"\"\"Complete training loop\"\"\"\n",
    "        best_val_acc = 0.0\n",
    "        best_model_state = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_acc = self.validate_epoch(val_loader)\n",
    "            \n",
    "            # Update learning rate\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['lr'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = self.model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "                print(f\"New best validation accuracy: {best_val_acc:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if early_stopping_patience and patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model_state:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def plot_training_history(self, save_path=None):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss\n",
    "        axes[0, 0].plot(self.history['train_loss'], label='Train Loss')\n",
    "        axes[0, 0].plot(self.history['val_loss'], label='Val Loss')\n",
    "        axes[0, 0].set_title('Model Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0, 1].plot(self.history['train_acc'], label='Train Acc')\n",
    "        axes[0, 1].plot(self.history['val_acc'], label='Val Acc')\n",
    "        axes[0, 1].set_title('Model Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Learning Rate\n",
    "        axes[1, 0].plot(self.history['lr'])\n",
    "        axes[1, 0].set_title('Learning Rate')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Remove empty subplot\n",
    "        axes[1, 1].remove()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation framework\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device=None):\n",
    "        self.model = model\n",
    "        self.device = device or DEVICE\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def evaluate(self, test_loader, return_predictions=False):\n",
    "        \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "                inputs = inputs.to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs).squeeze()\n",
    "                probabilities = torch.sigmoid(outputs)\n",
    "                predictions = (probabilities > 0.5).float()\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probabilities.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        all_predictions = np.array(all_predictions)\n",
    "        all_probabilities = np.array(all_probabilities)\n",
    "        all_labels = np.array(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results = self._calculate_metrics(all_labels, all_predictions, all_probabilities)\n",
    "        \n",
    "        if return_predictions:\n",
    "            results['predictions'] = all_predictions\n",
    "            results['probabilities'] = all_probabilities\n",
    "            results['labels'] = all_labels\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred, y_prob):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        # Basic metrics\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # ROC and PR curves\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_prob)\n",
    "        pr_auc = average_precision_score(y_true, y_prob)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'specificity': specificity,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'confusion_matrix': cm,\n",
    "            'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,\n",
    "            'fpr': fpr, 'tpr': tpr,\n",
    "            'precision_curve': precision_curve,\n",
    "            'recall_curve': recall_curve\n",
    "        }\n",
    "    \n",
    "    def plot_evaluation(self, results, model_name=\"Model\", save_path=None):\n",
    "        \"\"\"Plot comprehensive evaluation results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=results['confusion_matrix'],\n",
    "                                      display_labels=['Normal', 'Pneumonia'])\n",
    "        disp.plot(ax=axes[0, 0], cmap='Blues', values_format='d')\n",
    "        axes[0, 0].set_title(f'{model_name} - Confusion Matrix')\n",
    "        \n",
    "        # ROC Curve\n",
    "        axes[0, 1].plot(results['fpr'], results['tpr'], \n",
    "                       label=f'ROC Curve (AUC = {results[\"roc_auc\"]:.3f})')\n",
    "        axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "        axes[0, 1].set_xlabel('False Positive Rate')\n",
    "        axes[0, 1].set_ylabel('True Positive Rate')\n",
    "        axes[0, 1].set_title(f'{model_name} - ROC Curve')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Precision-Recall Curve\n",
    "        axes[1, 0].plot(results['recall_curve'], results['precision_curve'],\n",
    "                       label=f'PR Curve (AUC = {results[\"pr_auc\"]:.3f})')\n",
    "        axes[1, 0].set_xlabel('Recall')\n",
    "        axes[1, 0].set_ylabel('Precision')\n",
    "        axes[1, 0].set_title(f'{model_name} - Precision-Recall Curve')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Metrics Bar Plot\n",
    "        metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score']\n",
    "        values = [results['accuracy'], results['precision'], results['recall'], \n",
    "                 results['specificity'], results['f1_score']]\n",
    "        \n",
    "        bars = axes[1, 1].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
    "        axes[1, 1].set_ylabel('Score')\n",
    "        axes[1, 1].set_title(f'{model_name} - Performance Metrics')\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def print_detailed_results(self, results, model_name=\"Model\"):\n",
    "        \"\"\"Print detailed evaluation results\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{model_name} - Detailed Evaluation Results\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"\\nClassification Metrics:\")\n",
    "        print(f\"  Accuracy:    {results['accuracy']:.4f}\")\n",
    "        print(f\"  Precision:   {results['precision']:.4f}\")\n",
    "        print(f\"  Recall:      {results['recall']:.4f}\")\n",
    "        print(f\"  Specificity: {results['specificity']:.4f}\")\n",
    "        print(f\"  F1-Score:    {results['f1_score']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nAUC Scores:\")\n",
    "        print(f\"  ROC AUC:     {results['roc_auc']:.4f}\")\n",
    "        print(f\"  PR AUC:      {results['pr_auc']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(f\"  True Positives:  {results['tp']}\")\n",
    "        print(f\"  False Positives: {results['fp']}\")\n",
    "        print(f\"  True Negatives:  {results['tn']}\")\n",
    "        print(f\"  False Negatives: {results['fn']}\")\n",
    "        \n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(results['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_dict, test_loader, save_path=None):\n",
    "    \"\"\"\n",
    "    Compare multiple models on the same test set\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary of {model_name: model} pairs\n",
    "        test_loader: Test data loader\n",
    "        save_path: Path to save comparison results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Evaluating models...\")\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        evaluator = ModelEvaluator(model)\n",
    "        model_results = evaluator.evaluate(test_loader)\n",
    "        results[model_name] = model_results\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'roc_auc', 'pr_auc']\n",
    "    comparison_df = pd.DataFrame({\n",
    "        model_name: [model_results[metric] for metric in metrics]\n",
    "        for model_name, model_results in results.items()\n",
    "    }, index=metrics)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Metrics comparison\n",
    "    comparison_df.T.plot(kind='bar', ax=axes[0, 0], rot=45)\n",
    "    axes[0, 0].set_title('Model Performance Comparison')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ROC curves comparison\n",
    "    for model_name, model_results in results.items():\n",
    "        axes[0, 1].plot(model_results['fpr'], model_results['tpr'], \n",
    "                       label=f'{model_name} (AUC = {model_results[\"roc_auc\"]:.3f})')\n",
    "    axes[0, 1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[0, 1].set_xlabel('False Positive Rate')\n",
    "    axes[0, 1].set_ylabel('True Positive Rate')\n",
    "    axes[0, 1].set_title('ROC Curves Comparison')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # PR curves comparison\n",
    "    for model_name, model_results in results.items():\n",
    "        axes[1, 0].plot(model_results['recall_curve'], model_results['precision_curve'],\n",
    "                       label=f'{model_name} (AUC = {model_results[\"pr_auc\"]:.3f})')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Precision-Recall Curves Comparison')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # F1-Score vs Accuracy scatter\n",
    "    for model_name, model_results in results.items():\n",
    "        axes[1, 1].scatter(model_results['accuracy'], model_results['f1_score'], \n",
    "                          label=model_name, s=100)\n",
    "        axes[1, 1].annotate(model_name, \n",
    "                           (model_results['accuracy'], model_results['f1_score']),\n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1, 1].set_xlabel('Accuracy')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].set_title('Accuracy vs F1-Score')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Save results\n",
    "    if save_path:\n",
    "        results_file = save_path.replace('.png', '_results.json')\n",
    "        with open(results_file, 'w') as f:\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            json_results = {}\n",
    "            for model_name, model_results in results.items():\n",
    "                json_results[model_name] = {\n",
    "                    k: v.tolist() if isinstance(v, np.ndarray) else v\n",
    "                    for k, v in model_results.items()\n",
    "                    if k not in ['fpr', 'tpr', 'precision_curve', 'recall_curve', 'confusion_matrix']\n",
    "                }\n",
    "            json.dump(json_results, f, indent=2)\n",
    "        \n",
    "        comparison_df.to_csv(save_path.replace('.png', '_comparison.csv'))\n",
    "    \n",
    "    return results, comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pipeline(model_name, data_dir, config=None):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for a single model\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to train\n",
    "        data_dir: Path to the data directory\n",
    "        config: Training configuration dictionary\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        'batch_size': 32,\n",
    "        'image_size': 224,\n",
    "        'epochs': 25,\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 1e-5,\n",
    "        'early_stopping_patience': 5,\n",
    "        'val_split': 0.2\n",
    "    }\n",
    "    \n",
    "    if config:\n",
    "        default_config.update(config)\n",
    "    config = default_config\n",
    "    \n",
    "    print(f\"Training {model_name} with configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        data_dir, \n",
    "        batch_size=config['batch_size'],\n",
    "        image_size=config['image_size'],\n",
    "        val_split=config['val_split']\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                          lr=config['learning_rate'], \n",
    "                          weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(model, criterion, optimizer, scheduler=scheduler)\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train(\n",
    "        train_loader, val_loader, \n",
    "        epochs=config['epochs'],\n",
    "        early_stopping_patience=config['early_stopping_patience']\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluator = ModelEvaluator(model)\n",
    "    results = evaluator.evaluate(test_loader)\n",
    "    \n",
    "    # Print results\n",
    "    evaluator.print_detailed_results(results, model_name)\n",
    "    \n",
    "    # Plot results\n",
    "    trainer.plot_training_history(save_path=RESULTS_DIR / f\"{model_name}_training_history.png\")\n",
    "    evaluator.plot_evaluation(results, model_name, save_path=RESULTS_DIR / f\"{model_name}_evaluation.png\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = MODELS_DIR / f\"{model_name}_weights.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, history, results, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example Training and Evaluation\n",
    "\n",
    "Uncomment the cells below to run training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training experiments\n",
    "TRAINING_CONFIGS = {\n",
    "    'xception': {\n",
    "        'batch_size': 32,\n",
    "        'epochs': 20,\n",
    "        'learning_rate': 1e-4,\n",
    "        'early_stopping_patience': 5\n",
    "    },\n",
    "    'xception_lstm': {\n",
    "        'batch_size': 24,  # Smaller batch size for LSTM\n",
    "        'epochs': 25,\n",
    "        'learning_rate': 1e-4,\n",
    "        'early_stopping_patience': 7\n",
    "    },\n",
    "    'fusion': {\n",
    "        'batch_size': 24,  # Smaller batch size for fusion model\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 5e-5,  # Lower learning rate for fusion\n",
    "        'early_stopping_patience': 8\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Training configurations loaded.\")\n",
    "print(\"Uncomment the training cells below to start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train a single model\n",
    "# DATA_DIR = \"../data\"  # Adjust path to your data directory\n",
    "\n",
    "# model, history, results, test_loader = train_model_pipeline(\n",
    "#     model_name='xception',\n",
    "#     data_dir=DATA_DIR,\n",
    "#     config=TRAINING_CONFIGS['xception']\n",
    "# )\n",
    "\n",
    "print(\"Single model training pipeline is ready.\")\n",
    "print(\"Uncomment the above code and set DATA_DIR to train a model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Train and compare all models\n",
    "# DATA_DIR = \"../data\"  # Adjust path to your data directory\n",
    "\n",
    "# trained_models = {}\n",
    "# all_results = {}\n",
    "\n",
    "# for model_name in ['xception', 'xception_lstm', 'fusion']:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Training {model_name.upper()}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     \n",
    "#     model, history, results, test_loader = train_model_pipeline(\n",
    "#         model_name=model_name,\n",
    "#         data_dir=DATA_DIR,\n",
    "#         config=TRAINING_CONFIGS[model_name]\n",
    "#     )\n",
    "#     \n",
    "#     trained_models[model_name] = model\n",
    "#     all_results[model_name] = results\n",
    "\n",
    "# # Compare all models\n",
    "# comparison_results, comparison_df = compare_models(\n",
    "#     trained_models, \n",
    "#     test_loader, \n",
    "#     save_path=RESULTS_DIR / \"model_comparison.png\"\n",
    "# )\n",
    "\n",
    "print(\"Multi-model training and comparison pipeline is ready.\")\n",
    "print(\"Uncomment the above code and set DATA_DIR to train and compare all models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model_name, data_dir, k_folds=5, config=None):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for robust performance estimation\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to validate\n",
    "        data_dir: Path to the data directory\n",
    "        k_folds: Number of folds for cross-validation\n",
    "        config: Training configuration\n",
    "    \"\"\"\n",
    "    print(f\"Performing {k_folds}-fold cross-validation for {model_name}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    train_transform, _ = get_data_transforms(224)\n",
    "    dataset = PneumoniaDataset(data_dir, train_transform, 'train')\n",
    "    \n",
    "    # Prepare for stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.samples, dataset.labels)):\n",
    "        print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Create fold datasets\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Create and train model\n",
    "        model = get_model(model_name)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        trainer = ModelTrainer(model, criterion, optimizer)\n",
    "        history = trainer.train(train_loader, val_loader, epochs=10)  # Fewer epochs for CV\n",
    "        \n",
    "        # Evaluate fold\n",
    "        evaluator = ModelEvaluator(model)\n",
    "        val_results = evaluator.evaluate(val_loader)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'accuracy': val_results['accuracy'],\n",
    "            'precision': val_results['precision'],\n",
    "            'recall': val_results['recall'],\n",
    "            'f1_score': val_results['f1_score'],\n",
    "            'roc_auc': val_results['roc_auc']\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Results:\")\n",
    "        print(f\"  Accuracy: {val_results['accuracy']:.4f}\")\n",
    "        print(f\"  F1-Score: {val_results['f1_score']:.4f}\")\n",
    "        print(f\"  ROC AUC:  {val_results['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    cv_df = pd.DataFrame(fold_results)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{k_folds}-Fold Cross-Validation Results for {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
    "        mean_val = cv_df[metric].mean()\n",
    "        std_val = cv_df[metric].std()\n",
    "        print(f\"{metric.replace('_', ' ').title()::<12} {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "    \n",
    "    return cv_df\n",
    "\n",
    "# Example usage:\n",
    "# cv_results = cross_validate_model('xception', DATA_DIR, k_folds=5)\n",
    "\n",
    "print(\"Cross-validation framework is ready.\")\n",
    "print(\"Uncomment the example usage above to run cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "This comprehensive training and evaluation framework provides:\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Unified Training Pipeline**: Consistent training procedures across all model types\n",
    "2. **Comprehensive Evaluation**: Multiple metrics and visualizations\n",
    "3. **Model Comparison**: Side-by-side performance analysis\n",
    "4. **Cross-Validation**: Robust performance estimation\n",
    "5. **Reproducibility**: Fixed random seeds and structured configuration\n",
    "\n",
    "### Best Practices Implemented:\n",
    "\n",
    "1. **Data Augmentation**: Improves model generalization\n",
    "2. **Early Stopping**: Prevents overfitting\n",
    "3. **Learning Rate Scheduling**: Adaptive learning rate adjustment\n",
    "4. **Comprehensive Metrics**: Beyond accuracy for medical applications\n",
    "5. **Model Persistence**: Save and load trained models\n",
    "\n",
    "### Usage Guidelines:\n",
    "\n",
    "1. **Start Simple**: Begin with basic CNN models before complex architectures\n",
    "2. **Validate Properly**: Use stratified splits for class balance\n",
    "3. **Monitor Training**: Watch for overfitting and convergence\n",
    "4. **Compare Fairly**: Use same test set for all model comparisons\n",
    "5. **Document Results**: Save configurations and results for reproducibility\n",
    "\n",
    "### Medical AI Considerations:\n",
    "\n",
    "1. **Sensitivity vs Specificity**: Balance based on clinical requirements\n",
    "2. **Confidence Estimation**: Important for clinical decision support\n",
    "3. **Interpretability**: Consider model explainability for medical use\n",
    "4. **Robustness**: Test on diverse datasets and conditions\n",
    "\n",
    "This framework enables systematic development and evaluation of pneumonia detection models suitable for clinical research and applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}