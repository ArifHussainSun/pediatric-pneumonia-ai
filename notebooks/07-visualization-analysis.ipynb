{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Analysis for Pneumonia Detection\n",
    "\n",
    "This notebook provides comprehensive visualization and analysis tools for understanding model behavior and performance in pneumonia detection. It includes:\n",
    "\n",
    "1. **Data Exploration**: Visual analysis of the dataset characteristics\n",
    "2. **Model Interpretability**: Grad-CAM and feature visualization\n",
    "3. **Performance Analysis**: Detailed evaluation metrics and comparisons\n",
    "4. **Error Analysis**: Understanding model failures and edge cases\n",
    "5. **Clinical Insights**: Medical relevance and interpretation\n",
    "6. **Interactive Visualizations**: Tools for exploring model behavior\n",
    "\n",
    "These tools are essential for building trust in AI models for medical applications and understanding their clinical utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Image processing and visualization\n",
    "import cv2\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# PyTorch and deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "\n",
    "# Grad-CAM and interpretability\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, LayerCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# Scientific computing and metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from scipy import ndimage\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Interactive widgets (if available)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"ipywidgets not available. Interactive features will be disabled.\")\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create output directories\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "VISUALIZATIONS_DIR = RESULTS_DIR / \"visualizations\"\n",
    "VISUALIZATIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(data_dir):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of dataset characteristics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_stats = {'normal': 0, 'pneumonia': 0, 'total': 0}\n",
    "        \n",
    "        for class_name in ['normal', 'pneumonia']:\n",
    "            class_dir = Path(data_dir) / split / class_name\n",
    "            if class_dir.exists():\n",
    "                count = len(list(class_dir.glob('*.jpg')))\n",
    "                split_stats[class_name] = count\n",
    "                split_stats['total'] += count\n",
    "        \n",
    "        stats[split] = split_stats\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def plot_dataset_distribution(stats, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize dataset distribution across splits and classes\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Split distribution\n",
    "    splits = list(stats.keys())\n",
    "    split_totals = [stats[split]['total'] for split in splits]\n",
    "    \n",
    "    axes[0].pie(split_totals, labels=splits, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0].set_title('Dataset Split Distribution')\n",
    "    \n",
    "    # Class distribution per split\n",
    "    splits = list(stats.keys())\n",
    "    normal_counts = [stats[split]['normal'] for split in splits]\n",
    "    pneumonia_counts = [stats[split]['pneumonia'] for split in splits]\n",
    "    \n",
    "    x = np.arange(len(splits))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1].bar(x - width/2, normal_counts, width, label='Normal', alpha=0.8)\n",
    "    axes[1].bar(x + width/2, pneumonia_counts, width, label='Pneumonia', alpha=0.8)\n",
    "    axes[1].set_xlabel('Dataset Split')\n",
    "    axes[1].set_ylabel('Number of Images')\n",
    "    axes[1].set_title('Class Distribution by Split')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(splits)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overall class balance\n",
    "    total_normal = sum(stats[split]['normal'] for split in stats)\n",
    "    total_pneumonia = sum(stats[split]['pneumonia'] for split in stats)\n",
    "    \n",
    "    axes[2].pie([total_normal, total_pneumonia], \n",
    "               labels=['Normal', 'Pneumonia'], \n",
    "               autopct='%1.1f%%', \n",
    "               startangle=90,\n",
    "               colors=['lightblue', 'lightcoral'])\n",
    "    axes[2].set_title('Overall Class Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    for split, split_stats in stats.items():\n",
    "        print(f\"\\n{split.upper()} SET:\")\n",
    "        print(f\"  Normal: {split_stats['normal']:,}\")\n",
    "        print(f\"  Pneumonia: {split_stats['pneumonia']:,}\")\n",
    "        print(f\"  Total: {split_stats['total']:,}\")\n",
    "        \n",
    "        if split_stats['total'] > 0:\n",
    "            balance_ratio = split_stats['pneumonia'] / split_stats['normal']\n",
    "            print(f\"  Class Balance Ratio: {balance_ratio:.2f}\")\n",
    "\n",
    "def visualize_sample_images(data_dir, num_samples=8, save_path=None):\n",
    "    \"\"\"\n",
    "    Display sample images from each class\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 8))\n",
    "    \n",
    "    for class_idx, class_name in enumerate(['normal', 'pneumonia']):\n",
    "        class_dir = Path(data_dir) / 'train' / class_name\n",
    "        image_files = list(class_dir.glob('*.jpg'))[:num_samples]\n",
    "        \n",
    "        for idx, img_path in enumerate(image_files):\n",
    "            if idx < num_samples:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                axes[class_idx, idx].imshow(img, cmap='gray')\n",
    "                axes[class_idx, idx].set_title(f'{class_name.title()}\\n{img_path.name}')\n",
    "                axes[class_idx, idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Dataset', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Interpretability with Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAMVisualizer:\n",
    "    \"\"\"\n",
    "    Grad-CAM visualization for model interpretability\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layers=None):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Auto-detect target layers if not provided\n",
    "        if target_layers is None:\n",
    "            self.target_layers = self._get_target_layers()\n",
    "        else:\n",
    "            self.target_layers = target_layers\n",
    "        \n",
    "        # Initialize Grad-CAM\n",
    "        self.cam = GradCAM(model=model, target_layers=self.target_layers)\n",
    "    \n",
    "    def _get_target_layers(self):\n",
    "        \"\"\"\n",
    "        Automatically detect appropriate target layers for different model types\n",
    "        \"\"\"\n",
    "        # Look for common layer patterns\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'features' in name and isinstance(module, nn.Conv2d):\n",
    "                return [module]\n",
    "            elif 'layer4' in name and isinstance(module, nn.Conv2d):\n",
    "                return [module]\n",
    "            elif 'block' in name and isinstance(module, nn.Conv2d):\n",
    "                return [module]\n",
    "        \n",
    "        # Fallback: use the last convolutional layer\n",
    "        conv_layers = []\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                conv_layers.append(module)\n",
    "        \n",
    "        return [conv_layers[-1]] if conv_layers else []\n",
    "    \n",
    "    def generate_cam(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM visualization\n",
    "        \"\"\"\n",
    "        if target_class is None:\n",
    "            # Use the predicted class\n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)\n",
    "                predicted_class = (torch.sigmoid(output) > 0.5).int().item()\n",
    "            target_class = predicted_class\n",
    "        \n",
    "        targets = [ClassifierOutputTarget(target_class)]\n",
    "        grayscale_cam = self.cam(input_tensor=input_tensor, targets=targets)\n",
    "        \n",
    "        return grayscale_cam[0]  # Return first batch item\n",
    "    \n",
    "    def visualize_prediction(self, image_path, save_path=None):\n",
    "        \"\"\"\n",
    "        Complete visualization pipeline for a single image\n",
    "        \"\"\"\n",
    "        # Load and preprocess image\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Original image for visualization\n",
    "        original_img = Image.open(image_path).convert('RGB')\n",
    "        original_img = original_img.resize((224, 224))\n",
    "        original_array = np.array(original_img) / 255.0\n",
    "        \n",
    "        # Preprocessed image for model\n",
    "        input_tensor = transform(original_img).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "            probability = torch.sigmoid(output).item()\n",
    "            prediction = \"Pneumonia\" if probability > 0.5 else \"Normal\"\n",
    "            confidence = probability if probability > 0.5 else (1 - probability)\n",
    "        \n",
    "        # Generate Grad-CAM\n",
    "        cam = self.generate_cam(input_tensor)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Grad-CAM heatmap\n",
    "        axes[1].imshow(cam, cmap='jet', alpha=0.8)\n",
    "        axes[1].set_title('Grad-CAM Heatmap')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        cam_image = show_cam_on_image(original_array, cam, use_rgb=True)\n",
    "        axes[2].imshow(cam_image)\n",
    "        axes[2].set_title(f'Grad-CAM Overlay\\nPrediction: {prediction}\\nConfidence: {confidence:.3f}')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Model Interpretation: {Path(image_path).name}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'probability': probability,\n",
    "            'confidence': confidence,\n",
    "            'cam': cam\n",
    "        }\n",
    "    \n",
    "    def batch_visualize(self, image_paths, save_dir=None):\n",
    "        \"\"\"\n",
    "        Generate Grad-CAM visualizations for multiple images\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            print(f\"Processing {i+1}/{len(image_paths)}: {Path(image_path).name}\")\n",
    "            \n",
    "            save_path = None\n",
    "            if save_dir:\n",
    "                save_path = Path(save_dir) / f\"gradcam_{Path(image_path).stem}.png\"\n",
    "            \n",
    "            result = self.visualize_prediction(image_path, save_path)\n",
    "            result['image_path'] = image_path\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "\n",
    "def compare_gradcam_methods(model, image_path, save_path=None):\n",
    "    \"\"\"\n",
    "    Compare different Grad-CAM methods on the same image\n",
    "    \"\"\"\n",
    "    # Get target layers\n",
    "    target_layers = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            target_layers.append(module)\n",
    "    target_layers = [target_layers[-1]] if target_layers else []\n",
    "    \n",
    "    # Different CAM methods\n",
    "    cam_methods = {\n",
    "        'GradCAM': GradCAM,\n",
    "        'GradCAM++': GradCAMPlusPlus,\n",
    "        'ScoreCAM': ScoreCAM,\n",
    "        'XGradCAM': XGradCAM\n",
    "    }\n",
    "    \n",
    "    # Load image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    original_img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
    "    original_array = np.array(original_img) / 255.0\n",
    "    input_tensor = transform(original_img).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Generate CAMs\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for idx, (method_name, cam_class) in enumerate(cam_methods.items(), 1):\n",
    "        try:\n",
    "            cam = cam_class(model=model, target_layers=target_layers)\n",
    "            targets = [ClassifierOutputTarget(1)]  # Pneumonia class\n",
    "            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "            \n",
    "            cam_image = show_cam_on_image(original_array, grayscale_cam, use_rgb=True)\n",
    "            axes[idx].imshow(cam_image)\n",
    "            axes[idx].set_title(f'{method_name}')\n",
    "            axes[idx].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[idx].text(0.5, 0.5, f'Error: {method_name}\\n{str(e)[:50]}...', \n",
    "                          ha='center', va='center', transform=axes[idx].transAxes)\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    # Remove extra subplot\n",
    "    axes[5].remove()\n",
    "    \n",
    "    plt.suptitle(f'Grad-CAM Method Comparison: {Path(image_path).name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Analysis and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_performance_analysis(y_true, y_pred, y_prob, class_names=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Comprehensive performance analysis with multiple visualizations\n",
    "    \"\"\"\n",
    "    if class_names is None:\n",
    "        class_names = ['Normal', 'Pneumonia']\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = GridSpec(3, 4, figure=fig)\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    \n",
    "    # 2. ROC Curve\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax2.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    ax2.set_xlabel('False Positive Rate')\n",
    "    ax2.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('ROC Curve')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision-Recall Curve\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    ax3.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.3f})', linewidth=2)\n",
    "    ax3.set_xlabel('Recall')\n",
    "    ax3.set_ylabel('Precision')\n",
    "    ax3.set_title('Precision-Recall Curve')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Prediction Distribution\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    ax4.hist(y_prob[y_true == 0], bins=30, alpha=0.5, label='Normal', density=True)\n",
    "    ax4.hist(y_prob[y_true == 1], bins=30, alpha=0.5, label='Pneumonia', density=True)\n",
    "    ax4.axvline(x=0.5, color='red', linestyle='--', label='Threshold')\n",
    "    ax4.set_xlabel('Predicted Probability')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.set_title('Prediction Distribution')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Metrics by Threshold\n",
    "    ax5 = fig.add_subplot(gs[1, 0:2])\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    precisions, recalls, f1s, accuracies = [], [], [], []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_prob >= threshold).astype(int)\n",
    "        if len(np.unique(y_pred_thresh)) > 1:  # Avoid division by zero\n",
    "            precisions.append(precision_score(y_true, y_pred_thresh, zero_division=0))\n",
    "            recalls.append(recall_score(y_true, y_pred_thresh, zero_division=0))\n",
    "            f1s.append(f1_score(y_true, y_pred_thresh, zero_division=0))\n",
    "            accuracies.append(accuracy_score(y_true, y_pred_thresh))\n",
    "        else:\n",
    "            precisions.append(0)\n",
    "            recalls.append(0)\n",
    "            f1s.append(0)\n",
    "            accuracies.append(0)\n",
    "    \n",
    "    ax5.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "    ax5.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "    ax5.plot(thresholds, f1s, label='F1-Score', linewidth=2)\n",
    "    ax5.plot(thresholds, accuracies, label='Accuracy', linewidth=2)\n",
    "    ax5.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Default Threshold')\n",
    "    ax5.set_xlabel('Threshold')\n",
    "    ax5.set_ylabel('Metric Value')\n",
    "    ax5.set_title('Metrics vs Threshold')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Class-wise Performance\n",
    "    ax6 = fig.add_subplot(gs[1, 2:4])\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    normal_scores = [report['Normal'][metric] for metric in metrics]\n",
    "    pneumonia_scores = [report['Pneumonia'][metric] for metric in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax6.bar(x - width/2, normal_scores, width, label='Normal', alpha=0.8)\n",
    "    ax6.bar(x + width/2, pneumonia_scores, width, label='Pneumonia', alpha=0.8)\n",
    "    \n",
    "    ax6.set_xlabel('Metrics')\n",
    "    ax6.set_ylabel('Score')\n",
    "    ax6.set_title('Class-wise Performance')\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels([m.title() for m in metrics])\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    ax6.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (normal, pneumonia) in enumerate(zip(normal_scores, pneumonia_scores)):\n",
    "        ax6.text(i - width/2, normal + 0.01, f'{normal:.3f}', ha='center', va='bottom')\n",
    "        ax6.text(i + width/2, pneumonia + 0.01, f'{pneumonia:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 7. Error Analysis Summary\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # Calculate detailed metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    DETAILED PERFORMANCE METRICS\n",
    "    \n",
    "    Basic Metrics:\n",
    "    • Accuracy: {accuracy_score(y_true, y_pred):.4f}\n",
    "    • Precision: {precision_score(y_true, y_pred):.4f}\n",
    "    • Recall (Sensitivity): {recall_score(y_true, y_pred):.4f}\n",
    "    • Specificity: {tn/(tn+fp):.4f}\n",
    "    • F1-Score: {f1_score(y_true, y_pred):.4f}\n",
    "    \n",
    "    AUC Scores:\n",
    "    • ROC AUC: {roc_auc:.4f}\n",
    "    • PR AUC: {pr_auc:.4f}\n",
    "    \n",
    "    Confusion Matrix:\n",
    "    • True Positives (Pneumonia correctly identified): {tp}\n",
    "    • False Positives (Normal misclassified as Pneumonia): {fp}\n",
    "    • True Negatives (Normal correctly identified): {tn}\n",
    "    • False Negatives (Pneumonia misclassified as Normal): {fn}\n",
    "    \n",
    "    Clinical Relevance:\n",
    "    • Missed Pneumonia Cases: {fn} ({fn/(tp+fn)*100:.1f}% of actual pneumonia cases)\n",
    "    • False Alarms: {fp} ({fp/(tn+fp)*100:.1f}% of actual normal cases)\n",
    "    • Positive Predictive Value: {tp/(tp+fp):.4f}\n",
    "    • Negative Predictive Value: {tn/(tn+fn):.4f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax7.text(0.05, 0.95, metrics_text, transform=ax7.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Comprehensive Performance Analysis', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def analyze_model_errors(model, test_loader, save_dir=None, max_errors=10):\n",
    "    \"\"\"\n",
    "    Analyze and visualize model errors to understand failure cases\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    errors = {'false_positives': [], 'false_negatives': []}\n",
    "    correct_predictions = {'true_positives': [], 'true_negatives': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.sigmoid(outputs.squeeze())\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            for i in range(len(inputs)):\n",
    "                true_label = labels[i].item()\n",
    "                pred_label = predictions[i].item()\n",
    "                prob = probabilities[i].item()\n",
    "                \n",
    "                sample_info = {\n",
    "                    'image': inputs[i].cpu(),\n",
    "                    'true_label': true_label,\n",
    "                    'pred_label': pred_label,\n",
    "                    'probability': prob,\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'sample_idx': i\n",
    "                }\n",
    "                \n",
    "                if true_label == 0 and pred_label == 1:  # False positive\n",
    "                    errors['false_positives'].append(sample_info)\n",
    "                elif true_label == 1 and pred_label == 0:  # False negative\n",
    "                    errors['false_negatives'].append(sample_info)\n",
    "                elif true_label == 1 and pred_label == 1:  # True positive\n",
    "                    correct_predictions['true_positives'].append(sample_info)\n",
    "                elif true_label == 0 and pred_label == 0:  # True negative\n",
    "                    correct_predictions['true_negatives'].append(sample_info)\n",
    "    \n",
    "    # Visualize errors\n",
    "    for error_type, error_samples in errors.items():\n",
    "        if error_samples:\n",
    "            # Sort by confidence (probability distance from 0.5)\n",
    "            error_samples.sort(key=lambda x: abs(x['probability'] - 0.5), reverse=True)\n",
    "            \n",
    "            # Select top errors\n",
    "            top_errors = error_samples[:max_errors]\n",
    "            \n",
    "            # Create visualization\n",
    "            n_cols = min(5, len(top_errors))\n",
    "            n_rows = (len(top_errors) + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))\n",
    "            if n_rows == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for idx, error in enumerate(top_errors):\n",
    "                row, col = idx // n_cols, idx % n_cols\n",
    "                \n",
    "                # Denormalize image for visualization\n",
    "                img = error['image']\n",
    "                img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                img = torch.clamp(img, 0, 1)\n",
    "                img = img.permute(1, 2, 0).numpy()\n",
    "                \n",
    "                axes[row, col].imshow(img)\n",
    "                \n",
    "                true_class = \"Pneumonia\" if error['true_label'] == 1 else \"Normal\"\n",
    "                pred_class = \"Pneumonia\" if error['pred_label'] == 1 else \"Normal\"\n",
    "                \n",
    "                axes[row, col].set_title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {error[\"probability\"]:.3f}')\n",
    "                axes[row, col].axis('off')\n",
    "            \n",
    "            # Remove empty subplots\n",
    "            for idx in range(len(top_errors), n_rows * n_cols):\n",
    "                row, col = idx // n_cols, idx % n_cols\n",
    "                axes[row, col].remove()\n",
    "            \n",
    "            error_type_title = error_type.replace('_', ' ').title()\n",
    "            plt.suptitle(f'{error_type_title} Examples', fontsize=16)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_dir:\n",
    "                save_path = Path(save_dir) / f'{error_type}_analysis.png'\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "            plt.show()\n",
    "    \n",
    "    return errors, correct_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WIDGETS_AVAILABLE:\n",
    "    def create_interactive_threshold_widget(y_true, y_prob):\n",
    "        \"\"\"\n",
    "        Interactive widget for exploring different classification thresholds\n",
    "        \"\"\"\n",
    "        def update_metrics(threshold):\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "            \n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Metrics plot\n",
    "            metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1-Score']\n",
    "            values = [accuracy, precision, recall, specificity, f1]\n",
    "            \n",
    "            bars = axes[0].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
    "            axes[0].set_ylim(0, 1)\n",
    "            axes[0].set_title(f'Metrics at Threshold = {threshold:.2f}')\n",
    "            axes[0].set_ylabel('Score')\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, values):\n",
    "                axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                           f'{value:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            # Confusion matrix\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                       xticklabels=['Normal', 'Pneumonia'], \n",
    "                       yticklabels=['Normal', 'Pneumonia'], ax=axes[1])\n",
    "            axes[1].set_title('Confusion Matrix')\n",
    "            axes[1].set_ylabel('True Label')\n",
    "            axes[1].set_xlabel('Predicted Label')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print detailed info\n",
    "            print(f\"Threshold: {threshold:.2f}\")\n",
    "            print(f\"True Positives: {tp}, False Positives: {fp}\")\n",
    "            print(f\"True Negatives: {tn}, False Negatives: {fn}\")\n",
    "            print(f\"Missed Pneumonia Cases: {fn} ({fn/(tp+fn)*100:.1f}% of pneumonia cases)\")\n",
    "            print(f\"False Alarms: {fp} ({fp/(tn+fp)*100:.1f}% of normal cases)\")\n",
    "        \n",
    "        threshold_slider = widgets.FloatSlider(\n",
    "            value=0.5,\n",
    "            min=0.0,\n",
    "            max=1.0,\n",
    "            step=0.01,\n",
    "            description='Threshold:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        return widgets.interact(update_metrics, threshold=threshold_slider)\n",
    "    \n",
    "    def create_model_comparison_widget(models_results):\n",
    "        \"\"\"\n",
    "        Interactive widget for comparing different models\n",
    "        \"\"\"\n",
    "        def compare_models(metric):\n",
    "            model_names = list(models_results.keys())\n",
    "            metric_values = [models_results[model][metric] for model in model_names]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(model_names, metric_values, \n",
    "                          color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'][:len(model_names)])\n",
    "            \n",
    "            plt.title(f'Model Comparison: {metric.replace(\"_\", \" \").title()}')\n",
    "            plt.ylabel(metric.replace('_', ' ').title())\n",
    "            plt.xlabel('Models')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, value in zip(bars, metric_values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                        f'{value:.3f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Get available metrics\n",
    "        available_metrics = list(list(models_results.values())[0].keys())\n",
    "        metric_dropdown = widgets.Dropdown(\n",
    "            options=available_metrics,\n",
    "            value='accuracy',\n",
    "            description='Metric:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        return widgets.interact(compare_models, metric=metric_dropdown)\n",
    "\n",
    "else:\n",
    "    def create_interactive_threshold_widget(y_true, y_prob):\n",
    "        print(\"Interactive widgets not available. Install ipywidgets for interactive features.\")\n",
    "    \n",
    "    def create_model_comparison_widget(models_results):\n",
    "        print(\"Interactive widgets not available. Install ipywidgets for interactive features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clinical Relevance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_decision_analysis(y_true, y_prob, cost_matrix=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Analyze model performance from a clinical decision-making perspective\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_prob: Predicted probabilities\n",
    "        cost_matrix: 2x2 matrix of costs [TN, FP; FN, TP]\n",
    "    \"\"\"\n",
    "    if cost_matrix is None:\n",
    "        # Default cost matrix (clinical perspective)\n",
    "        # Missing pneumonia (FN) is much more costly than false alarm (FP)\n",
    "        cost_matrix = np.array([[0, 1],    # [TN, FP]\n",
    "                               [10, 0]])   # [FN, TP]\n",
    "    \n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    costs = []\n",
    "    metrics = {'sensitivity': [], 'specificity': [], 'ppv': [], 'npv': []}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Handle edge cases where only one class is predicted\n",
    "            if len(np.unique(y_pred)) == 1:\n",
    "                if y_pred[0] == 0:  # All predicted as negative\n",
    "                    tn, fp, fn, tp = len(y_true[y_true == 0]), 0, len(y_true[y_true == 1]), 0\n",
    "                else:  # All predicted as positive\n",
    "                    tn, fp, fn, tp = 0, len(y_true[y_true == 0]), 0, len(y_true[y_true == 1])\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Calculate cost\n",
    "        total_cost = (tn * cost_matrix[0, 0] + fp * cost_matrix[0, 1] +\n",
    "                     fn * cost_matrix[1, 0] + tp * cost_matrix[1, 1])\n",
    "        costs.append(total_cost)\n",
    "        \n",
    "        # Calculate clinical metrics\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        \n",
    "        metrics['sensitivity'].append(sensitivity)\n",
    "        metrics['specificity'].append(specificity)\n",
    "        metrics['ppv'].append(ppv)\n",
    "        metrics['npv'].append(npv)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    optimal_cost = costs[optimal_idx]\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Cost vs Threshold\n",
    "    axes[0, 0].plot(thresholds, costs, linewidth=2)\n",
    "    axes[0, 0].axvline(x=optimal_threshold, color='red', linestyle='--', \n",
    "                      label=f'Optimal Threshold: {optimal_threshold:.3f}')\n",
    "    axes[0, 0].axvline(x=0.5, color='gray', linestyle=':', alpha=0.7, label='Default (0.5)')\n",
    "    axes[0, 0].set_xlabel('Threshold')\n",
    "    axes[0, 0].set_ylabel('Total Cost')\n",
    "    axes[0, 0].set_title('Cost vs Classification Threshold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Sensitivity and Specificity\n",
    "    axes[0, 1].plot(thresholds, metrics['sensitivity'], label='Sensitivity (Recall)', linewidth=2)\n",
    "    axes[0, 1].plot(thresholds, metrics['specificity'], label='Specificity', linewidth=2)\n",
    "    axes[0, 1].axvline(x=optimal_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].axvline(x=0.5, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Threshold')\n",
    "    axes[0, 1].set_ylabel('Rate')\n",
    "    axes[0, 1].set_title('Sensitivity vs Specificity')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Predictive Values\n",
    "    axes[0, 2].plot(thresholds, metrics['ppv'], label='PPV (Precision)', linewidth=2)\n",
    "    axes[0, 2].plot(thresholds, metrics['npv'], label='NPV', linewidth=2)\n",
    "    axes[0, 2].axvline(x=optimal_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 2].axvline(x=0.5, color='gray', linestyle=':', alpha=0.7)\n",
    "    axes[0, 2].set_xlabel('Threshold')\n",
    "    axes[0, 2].set_ylabel('Predictive Value')\n",
    "    axes[0, 2].set_title('Positive vs Negative Predictive Value')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ROC Space with Operating Points\n",
    "    fpr_points = 1 - np.array(metrics['specificity'])\n",
    "    tpr_points = metrics['sensitivity']\n",
    "    \n",
    "    # Standard ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    axes[1, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})', linewidth=2)\n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'k--', alpha=0.7)\n",
    "    \n",
    "    # Mark optimal point\n",
    "    opt_fpr = fpr_points[optimal_idx]\n",
    "    opt_tpr = tpr_points[optimal_idx]\n",
    "    axes[1, 0].scatter([opt_fpr], [opt_tpr], color='red', s=100, zorder=5,\n",
    "                      label=f'Optimal Point (FPR={opt_fpr:.3f}, TPR={opt_tpr:.3f})')\n",
    "    \n",
    "    # Mark default threshold point\n",
    "    default_idx = 50  # Index for threshold 0.5\n",
    "    default_fpr = fpr_points[default_idx]\n",
    "    default_tpr = tpr_points[default_idx]\n",
    "    axes[1, 0].scatter([default_fpr], [default_tpr], color='gray', s=100, zorder=5,\n",
    "                      label=f'Default (0.5) Point')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "    axes[1, 0].set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    axes[1, 0].set_title('ROC Space with Operating Points')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Comparison of Thresholds\n",
    "    thresholds_to_compare = [0.3, 0.5, 0.7, optimal_threshold]\n",
    "    comparison_data = []\n",
    "    \n",
    "    for thresh in thresholds_to_compare:\n",
    "        y_pred_thresh = (y_prob >= thresh).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred_thresh)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Threshold': f'{thresh:.3f}',\n",
    "            'Sensitivity': sensitivity,\n",
    "            'Specificity': specificity,\n",
    "            'PPV': ppv,\n",
    "            'NPV': npv,\n",
    "            'Missed Cases': fn,\n",
    "            'False Alarms': fp\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot threshold comparison\n",
    "    x_pos = np.arange(len(thresholds_to_compare))\n",
    "    width = 0.15\n",
    "    \n",
    "    axes[1, 1].bar(x_pos - 1.5*width, comparison_df['Sensitivity'], width, label='Sensitivity', alpha=0.8)\n",
    "    axes[1, 1].bar(x_pos - 0.5*width, comparison_df['Specificity'], width, label='Specificity', alpha=0.8)\n",
    "    axes[1, 1].bar(x_pos + 0.5*width, comparison_df['PPV'], width, label='PPV', alpha=0.8)\n",
    "    axes[1, 1].bar(x_pos + 1.5*width, comparison_df['NPV'], width, label='NPV', alpha=0.8)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Threshold')\n",
    "    axes[1, 1].set_ylabel('Metric Value')\n",
    "    axes[1, 1].set_title('Threshold Comparison')\n",
    "    axes[1, 1].set_xticks(x_pos)\n",
    "    axes[1, 1].set_xticklabels(comparison_df['Threshold'])\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Clinical Impact Summary\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    # Calculate metrics for optimal threshold\n",
    "    y_pred_opt = (y_prob >= optimal_threshold).astype(int)\n",
    "    cm_opt = confusion_matrix(y_true, y_pred_opt)\n",
    "    tn_opt, fp_opt, fn_opt, tp_opt = cm_opt.ravel()\n",
    "    \n",
    "    clinical_summary = f\"\"\"\n",
    "    CLINICAL DECISION ANALYSIS SUMMARY\n",
    "    \n",
    "    Cost Matrix Used:\n",
    "    • True Negative (correct normal): {cost_matrix[0,0]}\n",
    "    • False Positive (false alarm): {cost_matrix[0,1]}\n",
    "    • False Negative (missed pneumonia): {cost_matrix[1,0]}\n",
    "    • True Positive (correct pneumonia): {cost_matrix[1,1]}\n",
    "    \n",
    "    Optimal Threshold: {optimal_threshold:.3f}\n",
    "    Total Cost at Optimal: {optimal_cost:.0f}\n",
    "    \n",
    "    Clinical Performance:\n",
    "    • Sensitivity: {tp_opt/(tp_opt+fn_opt):.1%} ({tp_opt}/{tp_opt+fn_opt} pneumonia cases detected)\n",
    "    • Specificity: {tn_opt/(tn_opt+fp_opt):.1%} ({tn_opt}/{tn_opt+fp_opt} normal cases correct)\n",
    "    • Missed Pneumonia: {fn_opt} cases ({fn_opt/(tp_opt+fn_opt)*100:.1f}%)\n",
    "    • False Alarms: {fp_opt} cases ({fp_opt/(tn_opt+fp_opt)*100:.1f}%)\n",
    "    \n",
    "    Clinical Interpretation:\n",
    "    • PPV: {tp_opt/(tp_opt+fp_opt)*100:.1f}% of positive predictions are correct\n",
    "    • NPV: {tn_opt/(tn_opt+fn_opt)*100:.1f}% of negative predictions are correct\n",
    "    \n",
    "    Recommendation:\n",
    "    Use threshold {optimal_threshold:.3f} for clinical deployment\n",
    "    to minimize cost-weighted classification errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 2].text(0.05, 0.95, clinical_summary, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('Clinical Decision Analysis', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'optimal_cost': optimal_cost,\n",
    "        'threshold_comparison': comparison_df,\n",
    "        'cost_curve': costs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Usage and Demonstrations\n",
    "\n",
    "Uncomment the cells below to run visualizations when models and data are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Dataset Analysis\n",
    "# DATA_DIR = \"../data\"  # Adjust path to your data directory\n",
    "\n",
    "# # Analyze dataset statistics\n",
    "# stats = analyze_dataset_statistics(DATA_DIR)\n",
    "# plot_dataset_distribution(stats, save_path=VISUALIZATIONS_DIR / \"dataset_distribution.png\")\n",
    "\n",
    "# # Visualize sample images\n",
    "# visualize_sample_images(DATA_DIR, num_samples=8, \n",
    "#                        save_path=VISUALIZATIONS_DIR / \"sample_images.png\")\n",
    "\n",
    "print(\"Dataset analysis functions are ready.\")\n",
    "print(\"Uncomment the above code and set DATA_DIR to analyze your dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Model Interpretability with Grad-CAM\n",
    "# MODEL_PATH = \"../models/xception_weights.pth\"  # Adjust path to your model\n",
    "# SAMPLE_IMAGE = \"../data/test/pneumonia/sample.jpg\"  # Adjust to sample image\n",
    "\n",
    "# # Load model (example for Xception)\n",
    "# from notebooks.models import XceptionModel  # Import your model class\n",
    "# model = XceptionModel()\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "# model.to(DEVICE)\n",
    "\n",
    "# # Create Grad-CAM visualizer\n",
    "# gradcam_viz = GradCAMVisualizer(model)\n",
    "\n",
    "# # Visualize single prediction\n",
    "# result = gradcam_viz.visualize_prediction(SAMPLE_IMAGE, \n",
    "#                                          save_path=VISUALIZATIONS_DIR / \"gradcam_example.png\")\n",
    "\n",
    "# # Compare different Grad-CAM methods\n",
    "# compare_gradcam_methods(model, SAMPLE_IMAGE, \n",
    "#                        save_path=VISUALIZATIONS_DIR / \"gradcam_comparison.png\")\n",
    "\n",
    "print(\"Grad-CAM visualization functions are ready.\")\n",
    "print(\"Uncomment the above code and set paths to visualize model interpretability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Performance Analysis\n",
    "# # Assuming you have model predictions\n",
    "# y_true = np.array([0, 1, 1, 0, 1, 0, 0, 1])  # Example true labels\n",
    "# y_pred = np.array([0, 1, 0, 0, 1, 1, 0, 1])  # Example predictions\n",
    "# y_prob = np.array([0.2, 0.8, 0.4, 0.3, 0.9, 0.6, 0.1, 0.7])  # Example probabilities\n",
    "\n",
    "# # Detailed performance analysis\n",
    "# detailed_performance_analysis(y_true, y_pred, y_prob, \n",
    "#                              save_path=VISUALIZATIONS_DIR / \"performance_analysis.png\")\n",
    "\n",
    "# # Clinical decision analysis\n",
    "# clinical_results = clinical_decision_analysis(y_true, y_prob,\n",
    "#                                              save_path=VISUALIZATIONS_DIR / \"clinical_analysis.png\")\n",
    "\n",
    "print(\"Performance analysis functions are ready.\")\n",
    "print(\"Uncomment the above code with your model predictions to analyze performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Interactive Widgets (if available)\n",
    "# if WIDGETS_AVAILABLE:\n",
    "#     # Interactive threshold exploration\n",
    "#     print(\"Interactive Threshold Explorer:\")\n",
    "#     create_interactive_threshold_widget(y_true, y_prob)\n",
    "    \n",
    "#     # Model comparison widget\n",
    "#     models_results = {\n",
    "#         'xception': {'accuracy': 0.92, 'precision': 0.89, 'recall': 0.94, 'f1_score': 0.91},\n",
    "#         'fusion': {'accuracy': 0.94, 'precision': 0.91, 'recall': 0.96, 'f1_score': 0.93},\n",
    "#         'ensemble': {'accuracy': 0.95, 'precision': 0.93, 'recall': 0.97, 'f1_score': 0.95}\n",
    "#     }\n",
    "#     print(\"\\nInteractive Model Comparison:\")\n",
    "#     create_model_comparison_widget(models_results)\n",
    "# else:\n",
    "#     print(\"Install ipywidgets for interactive features\")\n",
    "\n",
    "print(\"Interactive visualization functions are ready.\")\n",
    "print(\"Uncomment the above code to create interactive widgets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization Summary and Best Practices\n",
    "\n",
    "This comprehensive visualization and analysis framework provides essential tools for understanding and validating pneumonia detection models:\n",
    "\n",
    "### Key Visualization Categories:\n",
    "\n",
    "1. **Data Exploration**:\n",
    "   - Dataset distribution analysis\n",
    "   - Class balance visualization\n",
    "   - Sample image inspection\n",
    "\n",
    "2. **Model Interpretability**:\n",
    "   - Grad-CAM heatmaps showing model attention\n",
    "   - Comparison of different CAM methods\n",
    "   - Feature importance visualization\n",
    "\n",
    "3. **Performance Assessment**:\n",
    "   - Comprehensive metric analysis\n",
    "   - ROC and Precision-Recall curves\n",
    "   - Confusion matrix analysis\n",
    "   - Threshold optimization\n",
    "\n",
    "4. **Error Analysis**:\n",
    "   - False positive/negative case studies\n",
    "   - Failure pattern identification\n",
    "   - Edge case analysis\n",
    "\n",
    "5. **Clinical Relevance**:\n",
    "   - Cost-sensitive analysis\n",
    "   - Clinical decision thresholds\n",
    "   - Sensitivity vs specificity trade-offs\n",
    "\n",
    "### Best Practices for Medical AI Visualization:\n",
    "\n",
    "1. **Interpretability First**: Always provide explanations for model decisions\n",
    "2. **Clinical Context**: Frame results in terms of clinical impact\n",
    "3. **Uncertainty Quantification**: Show confidence levels and uncertainty\n",
    "4. **Comprehensive Metrics**: Use multiple evaluation metrics\n",
    "5. **Error Analysis**: Understand and communicate failure modes\n",
    "6. **Threshold Optimization**: Consider clinical costs in threshold selection\n",
    "7. **Interactive Exploration**: Enable dynamic analysis when possible\n",
    "\n",
    "### Clinical Considerations:\n",
    "\n",
    "- **Sensitivity vs Specificity**: Balance based on clinical requirements\n",
    "- **False Negative Cost**: Missing pneumonia cases is typically more costly\n",
    "- **Interpretability**: Clinicians need to understand model reasoning\n",
    "- **Robustness**: Models must work across diverse patient populations\n",
    "- **Integration**: Visualizations should fit clinical workflows\n",
    "\n",
    "This framework enables thorough analysis and validation of pneumonia detection models, ensuring they meet both technical and clinical requirements for safe deployment in medical settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}