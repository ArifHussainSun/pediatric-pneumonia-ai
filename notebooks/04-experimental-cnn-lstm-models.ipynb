{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental CNN-LSTM Models for Pediatric Pneumonia Detection\n",
    "\n",
    "## Overview\n",
    "This notebook explores experimental CNN-LSTM architectures that combine convolutional feature extraction with sequential processing:\n",
    "- **Xception-LSTM**: Advanced CNN backbone with LSTM for spatial token processing\n",
    "- **Custom CNN-LSTM**: Simple 3-layer CNN with LSTM for comparison\n",
    "- **Grad-CAM Visualization**: Understanding what these hybrid models focus on\n",
    "\n",
    "## Research Background\n",
    "This work investigates whether adding sequential processing (LSTM) to CNNs can improve pneumonia detection by:\n",
    "- **Spatial Token Analysis**: Treating image regions as sequential tokens\n",
    "- **Contextual Understanding**: LSTM learns relationships between different lung regions\n",
    "- **Attention Mechanisms**: LSTM can focus on relevant spatial areas\n",
    "\n",
    "## Key Concepts\n",
    "- **CNN-LSTM Hybrid**: Combines spatial feature extraction with sequential processing\n",
    "- **Spatial Tokens**: Image regions (7x7 grid = 49 tokens) processed sequentially\n",
    "- **Sequential Learning**: LSTM learns patterns across spatial locations\n",
    "- **Feature Maps as Sequences**: Converting 2D feature maps to 1D token sequences\n",
    "\n",
    "## Experimental Nature\n",
    "These models are **research prototypes** exploring whether sequential processing adds value to medical image analysis. Results may vary and require further validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # For advanced CNN backbones\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2  # For Grad-CAM visualization\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device and random seeds\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experimental Configuration\n",
    "\n",
    "**Research Parameters:**\n",
    "- **Small Batch Size**: 8-32 for detailed gradient analysis\n",
    "- **Extended Training**: 20 epochs to observe convergence patterns\n",
    "- **Dropout Experimentation**: Various dropout rates for regularization\n",
    "- **LSTM Hidden Units**: 512 for rich sequential representations\n",
    "\n",
    "**Spatial Token Concept:**\n",
    "- **7x7 Spatial Grid**: Divides feature maps into 49 spatial tokens\n",
    "- **Sequential Processing**: LSTM processes tokens in spatial order\n",
    "- **Contextual Learning**: LSTM learns relationships between lung regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental configuration\n",
    "CONFIG = {\n",
    "    'batch_size': 8,           # Small batch for detailed analysis\n",
    "    'test_batch_size': 64,     # Larger batch for efficient testing\n",
    "    'epochs': 20,              # Extended training for research\n",
    "    'image_size': 224,         # Standard input size\n",
    "    'lstm_units': 512,         # Rich sequential representation\n",
    "    'dense_units': 128,        # Classifier hidden units\n",
    "    'dropout_rate': 0.3,       # Regularization\n",
    "    'learning_rate': 1e-4,     # Conservative learning rate\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Data directories\n",
    "TRAIN_DIR = '../data/training/augmented_train'\n",
    "TEST_DIR = '../data/testing/augmented_test'\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets for experimental analysis...\")\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=transform)\n",
    "\n",
    "# Create data loaders with experimental batch sizes\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['test_batch_size'], shuffle=False)\n",
    "\n",
    "# Dataset analysis\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "# Analyze class distribution\n",
    "train_counts = Counter(train_dataset.targets)\n",
    "test_counts = Counter(test_dataset.targets)\n",
    "print(f\"\\nTrain distribution: {dict(train_counts)}\")\n",
    "print(f\"Test distribution: {dict(test_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Xception-LSTM Architecture\n",
    "\n",
    "### Advanced CNN-LSTM Hybrid\n",
    "\n",
    "**Architecture Innovation:**\n",
    "1. **Xception Backbone**: Extract rich spatial features (2048 channels)\n",
    "2. **Spatial Tokenization**: Convert 7x7 feature map to 49 spatial tokens\n",
    "3. **LSTM Processing**: Sequential analysis of spatial relationships\n",
    "4. **Feature Integration**: Combine spatial and sequential information\n",
    "\n",
    "**Key Research Questions:**\n",
    "- Can LSTM improve spatial understanding beyond standard CNNs?\n",
    "- Do sequential patterns exist in lung X-ray spatial features?\n",
    "- How does spatial token order affect pneumonia detection?\n",
    "\n",
    "**Technical Details:**\n",
    "- **Input**: (Batch, 3, 224, 224) X-ray images\n",
    "- **Xception Output**: (Batch, 2048, 7, 7) feature maps\n",
    "- **Spatial Tokens**: (Batch, 49, 2048) sequential representation\n",
    "- **LSTM Output**: (Batch, 512) contextual features\n",
    "- **Final Prediction**: Binary classification (pneumonia/normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Experimental Xception-LSTM architecture for pneumonia detection.\n",
    "    \n",
    "    This model combines the feature extraction power of Xception with\n",
    "    the sequential processing capabilities of LSTM for spatial analysis.\n",
    "    \n",
    "    Research Innovation:\n",
    "    - Treats spatial feature maps as sequences of tokens\n",
    "    - Uses LSTM to learn relationships between different lung regions\n",
    "    - Investigates whether sequential processing improves medical image analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, freeze_layers=100, use_channel_reduction=False, reduced_dim=512):\n",
    "        \"\"\"\n",
    "        Initialize the experimental Xception-LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            freeze_layers: Number of early Xception layers to freeze\n",
    "            use_channel_reduction: Whether to reduce channel dimensions for efficiency\n",
    "            reduced_dim: Target dimensions if using channel reduction\n",
    "        \"\"\"\n",
    "        super(XceptionLSTM, self).__init__()\n",
    "        \n",
    "        # ========== Xception Backbone ==========\n",
    "        # Use Xception for feature extraction only (no classification head)\n",
    "        self.xception = timm.create_model(\"xception\", pretrained=True, features_only=True)\n",
    "        \n",
    "        # Freeze early layers for stable training on medical data\n",
    "        if freeze_layers > 0:\n",
    "            for name, param in list(self.xception.named_parameters())[:freeze_layers]:\n",
    "                param.requires_grad = False\n",
    "            print(f\"Frozen first {freeze_layers} Xception layers for stability\")\n",
    "        \n",
    "        # ========== Channel Reduction (Optional) ==========\n",
    "        # Reduce computational complexity while maintaining performance\n",
    "        self.use_channel_reduction = use_channel_reduction\n",
    "        if use_channel_reduction:\n",
    "            self.channel_reducer = nn.Sequential(\n",
    "                nn.Conv2d(2048, reduced_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(reduced_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            lstm_input_dim = reduced_dim\n",
    "            print(f\"Using channel reduction: 2048 -> {reduced_dim} dimensions\")\n",
    "        else:\n",
    "            lstm_input_dim = 2048\n",
    "            print(\"Using full 2048-dimensional features\")\n",
    "        \n",
    "        # ========== LSTM Sequential Processor ==========\n",
    "        # Process spatial tokens sequentially to learn regional relationships\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=lstm_input_dim,\n",
    "            hidden_size=CONFIG['lstm_units'],\n",
    "            batch_first=True,\n",
    "            dropout=0.2 if CONFIG['epochs'] > 10 else 0.0  # LSTM dropout for longer training\n",
    "        )\n",
    "        \n",
    "        # ========== Classification Head ==========\n",
    "        # Transform LSTM output to final prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(CONFIG['lstm_units'], CONFIG['dense_units']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CONFIG['dropout_rate']),\n",
    "            nn.Linear(CONFIG['dense_units'], 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        print(f\"Model initialized with {self._count_parameters():,} trainable parameters\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the experimental architecture.\n",
    "        \n",
    "        Args:\n",
    "            x: Input batch [batch_size, 3, 224, 224]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Raw prediction scores [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # ========== Feature Extraction ==========\n",
    "        # Extract spatial features using Xception backbone\n",
    "        features = self.xception(x)[-1]  # Take last feature stage: [batch, 2048, 7, 7]\n",
    "        \n",
    "        # ========== Optional Channel Reduction ==========\n",
    "        if self.use_channel_reduction:\n",
    "            features = self.channel_reducer(features)  # [batch, reduced_dim, 7, 7]\n",
    "        \n",
    "        # ========== Spatial Tokenization ==========\n",
    "        # Convert 2D feature maps to sequence of spatial tokens\n",
    "        batch_size, channels, height, width = features.shape\n",
    "        \n",
    "        # Flatten spatial dimensions and transpose for LSTM input\n",
    "        # [batch, channels, height*width] -> [batch, height*width, channels]\n",
    "        spatial_tokens = features.flatten(2).permute(0, 2, 1)\n",
    "        # Result: [batch_size, 49, channels] - 49 spatial tokens per image\n",
    "        \n",
    "        # ========== Sequential Processing ==========\n",
    "        # Process spatial tokens through LSTM to learn regional relationships\n",
    "        lstm_output, (hidden_state, _) = self.lstm(spatial_tokens)\n",
    "        \n",
    "        # Use final hidden state as global representation\n",
    "        # hidden_state shape: [1, batch_size, lstm_units]\n",
    "        global_features = hidden_state.squeeze(0)  # [batch_size, lstm_units]\n",
    "        \n",
    "        # ========== Classification ==========\n",
    "        # Transform global features to final prediction\n",
    "        logits = self.classifier(global_features)  # [batch_size, 1]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _count_parameters(self):\n",
    "        \"\"\"Count trainable parameters for model analysis.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def get_spatial_attention(self, x):\n",
    "        \"\"\"\n",
    "        Experimental method to analyze spatial attention patterns.\n",
    "        Returns LSTM attention weights across spatial tokens.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.xception(x)[-1]\n",
    "            if self.use_channel_reduction:\n",
    "                features = self.channel_reducer(features)\n",
    "            \n",
    "            spatial_tokens = features.flatten(2).permute(0, 2, 1)\n",
    "            lstm_output, _ = self.lstm(spatial_tokens)\n",
    "            \n",
    "            # Calculate attention as magnitude of LSTM outputs\n",
    "            attention = torch.norm(lstm_output, dim=2)  # [batch, num_tokens]\n",
    "            attention = torch.softmax(attention, dim=1)  # Normalize to probabilities\n",
    "            \n",
    "            return attention.reshape(-1, 7, 7)  # Reshape to spatial dimensions\n",
    "\n",
    "print(\"Xception-LSTM experimental architecture defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom CNN-LSTM Architecture\n",
    "\n",
    "### Simple CNN-LSTM for Comparison\n",
    "\n",
    "**Research Purpose:**\n",
    "- Compare against the Xception-LSTM to understand contribution of backbone complexity\n",
    "- Investigate whether simple CNN + LSTM can achieve competitive performance\n",
    "- Analyze trade-offs between model complexity and accuracy\n",
    "\n",
    "**Architecture:**\n",
    "1. **3-Layer CNN**: Basic feature extraction (200->150->100 channels)\n",
    "2. **Spatial Pooling**: Reduce to 7x7 to match Xception-LSTM token count\n",
    "3. **LSTM Processing**: Same sequential approach as Xception-LSTM\n",
    "4. **Lightweight Classifier**: Smaller hidden dimensions for efficiency\n",
    "\n",
    "**Advantages:**\n",
    "- **Faster Training**: Fewer parameters, quicker convergence\n",
    "- **Lower Memory**: Suitable for resource-constrained environments\n",
    "- **Interpretability**: Simpler architecture easier to understand\n",
    "- **Baseline Comparison**: Establishes minimum performance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNNLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 3-layer CNN + LSTM architecture for pneumonia detection.\n",
    "    \n",
    "    This lightweight model serves as a comparison baseline to understand\n",
    "    whether complex backbones are necessary for CNN-LSTM architectures.\n",
    "    \n",
    "    Research Questions:\n",
    "    - Can simple CNN + LSTM compete with advanced architectures?\n",
    "    - What's the minimum complexity needed for effective pneumonia detection?\n",
    "    - How do computational requirements scale with performance?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.27, lstm_hidden=50):\n",
    "        \"\"\"\n",
    "        Initialize the lightweight CNN-LSTM model.\n",
    "        \n",
    "        Args:\n",
    "            dropout_rate: Dropout probability for regularization\n",
    "            lstm_hidden: LSTM hidden units (smaller than Xception-LSTM)\n",
    "        \"\"\"\n",
    "        super(CustomCNNLSTM, self).__init__()\n",
    "        \n",
    "        # ========== Simple CNN Backbone ==========\n",
    "        # 3-layer CNN for basic feature extraction\n",
    "        self.cnn_backbone = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(3, 200, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # Reduce spatial dimensions\n",
    "            \n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(200, 150, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # Further spatial reduction\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(150, 100, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Adaptive pooling to ensure 7x7 output (same as Xception-LSTM)\n",
    "            nn.AdaptiveAvgPool2d((7, 7))\n",
    "        )\n",
    "        \n",
    "        # ========== Sequential Processor ==========\n",
    "        # LSTM for processing spatial tokens\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=100,           # CNN output channels\n",
    "            hidden_size=lstm_hidden,  # Smaller than Xception-LSTM\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # ========== Classification Head ==========\n",
    "        # Lightweight classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)  # Binary classification\n",
    "        )\n",
    "        \n",
    "        print(f\"Lightweight CNN-LSTM initialized with {self._count_parameters():,} parameters\")\n",
    "        print(f\"This is significantly fewer than Xception-LSTM for comparison\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the lightweight architecture.\n",
    "        \n",
    "        Args:\n",
    "            x: Input batch [batch_size, 3, 224, 224]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Raw prediction scores [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # ========== Feature Extraction ==========\n",
    "        # Extract features using simple CNN\n",
    "        cnn_features = self.cnn_backbone(x)  # [batch_size, 100, 7, 7]\n",
    "        \n",
    "        # ========== Spatial Tokenization ==========\n",
    "        # Convert to spatial tokens (same approach as Xception-LSTM)\n",
    "        batch_size, channels, height, width = cnn_features.shape\n",
    "        \n",
    "        # Flatten and transpose: [batch, channels, H*W] -> [batch, H*W, channels]\n",
    "        spatial_tokens = cnn_features.flatten(2).permute(0, 2, 1)\n",
    "        # Result: [batch_size, 49, 100] - same token count, fewer features per token\n",
    "        \n",
    "        # ========== Sequential Processing ==========\n",
    "        # Process spatial tokens through LSTM\n",
    "        lstm_output, (hidden_state, _) = self.lstm(spatial_tokens)\n",
    "        \n",
    "        # Use final hidden state\n",
    "        global_features = hidden_state.squeeze(0)  # [batch_size, lstm_hidden]\n",
    "        \n",
    "        # ========== Classification ==========\n",
    "        logits = self.classifier(global_features)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _count_parameters(self):\n",
    "        \"\"\"Count trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Custom CNN-LSTM comparison architecture defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Training Framework\n",
    "\n",
    "**Research-Oriented Training:**\n",
    "- **Extended Epochs**: 20 epochs to observe convergence patterns\n",
    "- **Detailed Logging**: Track loss, accuracy, and learning dynamics\n",
    "- **Model Comparison**: Train both architectures with identical settings\n",
    "- **Performance Analysis**: Compare efficiency vs accuracy trade-offs\n",
    "\n",
    "**Training Considerations:**\n",
    "- **BCEWithLogitsLoss**: Numerically stable for binary classification\n",
    "- **Adam Optimizer**: Adaptive learning rates for complex architectures\n",
    "- **Gradient Clipping**: Prevent exploding gradients in LSTM components\n",
    "- **Learning Rate Scheduling**: Optional for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_model(model, train_loader, test_loader, num_epochs, learning_rate, model_name):\n",
    "    \"\"\"\n",
    "    Train experimental CNN-LSTM models with detailed analysis.\n",
    "    \n",
    "    Args:\n",
    "        model: CNN-LSTM model to train\n",
    "        train_loader: Training data loader\n",
    "        test_loader: Testing data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimization\n",
    "        model_name: Name for logging and identification\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (trained_model, detailed_history)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup training components\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Stable binary classification\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), \n",
    "        lr=learning_rate,\n",
    "        weight_decay=1e-5  # L2 regularization\n",
    "    )\n",
    "    \n",
    "    # Track detailed training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'epoch_times': [],\n",
    "        'parameter_count': sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nStarting experimental training: {model_name}\")\n",
    "    print(f\"Model parameters: {history['parameter_count']:,}\")\n",
    "    print(f\"Training configuration: {num_epochs} epochs, LR={learning_rate}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # ========== Training Phase ==========\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1:2d}/{num_epochs} [Train]', leave=False)\n",
    "        \n",
    "        for inputs, labels in train_pbar:\n",
    "            # Prepare data for BCEWithLogitsLoss\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Prevent exploding gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct_train += (predictions == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct_train/total_train:.1f}%'\n",
    "            })\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        train_accuracy = 100. * correct_train / total_train\n",
    "        \n",
    "        # ========== Evaluation Phase ==========\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_pbar = tqdm(test_loader, desc=f'Epoch {epoch+1:2d}/{num_epochs} [Test]', leave=False)\n",
    "            \n",
    "            for inputs, labels in test_pbar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.float().unsqueeze(1).to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                \n",
    "                correct_test += (predictions == labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "                \n",
    "                test_pbar.set_postfix({\n",
    "                    'Acc': f'{100.*correct_test/total_test:.1f}%'\n",
    "                })\n",
    "        \n",
    "        test_accuracy = 100. * correct_test / total_test\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['test_acc'].append(test_accuracy)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch [{epoch+1:2d}/{num_epochs}] - '\n",
    "              f'Loss: {epoch_loss:.4f}, '\n",
    "              f'Train: {train_accuracy:.2f}%, '\n",
    "              f'Test: {test_accuracy:.2f}%, '\n",
    "              f'Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    # Training summary\n",
    "    avg_epoch_time = np.mean(history['epoch_times'])\n",
    "    best_test_acc = max(history['test_acc'])\n",
    "    final_test_acc = history['test_acc'][-1]\n",
    "    \n",
    "    print(f\"\\n{model_name} Training Summary:\")\n",
    "    print(f\"Best test accuracy: {best_test_acc:.2f}%\")\n",
    "    print(f\"Final test accuracy: {final_test_acc:.2f}%\")\n",
    "    print(f\"Average epoch time: {avg_epoch_time:.1f}s\")\n",
    "    print(f\"Total training time: {sum(history['epoch_times']):.1f}s\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Experimental training framework defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Experimental Models\n",
    "\n",
    "### 5.1 Train Xception-LSTM (Advanced Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Xception-LSTM model\n",
    "print(\"Initializing Xception-LSTM experimental model...\")\n",
    "xception_lstm = XceptionLSTM(\n",
    "    freeze_layers=100,\n",
    "    use_channel_reduction=False  # Use full features for maximum performance\n",
    ")\n",
    "\n",
    "# Train the advanced model\n",
    "trained_xception_lstm, xception_lstm_history = train_experimental_model(\n",
    "    model=xception_lstm,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=CONFIG['epochs'],\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    model_name=\"Xception-LSTM\"\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_xception_lstm.state_dict(), '../models/experimental_xception_lstm.pth')\n",
    "print(\"Xception-LSTM model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Custom CNN-LSTM (Lightweight Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Custom CNN-LSTM model\n",
    "print(\"\\nInitializing Custom CNN-LSTM comparison model...\")\n",
    "custom_cnn_lstm = CustomCNNLSTM(\n",
    "    dropout_rate=0.27,\n",
    "    lstm_hidden=50\n",
    ")\n",
    "\n",
    "# Train the lightweight model\n",
    "trained_custom_lstm, custom_lstm_history = train_experimental_model(\n",
    "    model=custom_cnn_lstm,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=CONFIG['epochs'],\n",
    "    learning_rate=6e-4,  # Slightly higher LR for simpler model\n",
    "    model_name=\"Custom CNN-LSTM\"\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_custom_lstm.state_dict(), '../models/experimental_custom_cnn_lstm.pth')\n",
    "print(\"Custom CNN-LSTM model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experimental_model(model, test_loader, class_names, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of experimental CNN-LSTM models.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
    "    \n",
    "    # Convert to arrays\n",
    "    y_true = np.array(all_labels, dtype=int)\n",
    "    y_pred = np.array(all_predictions, dtype=int)\n",
    "    y_prob = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (y_true == y_pred).mean() * 100\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, \n",
    "                                 target_names=class_names, \n",
    "                                 output_dict=True)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'probabilities': y_prob\n",
    "    }\n",
    "\n",
    "# Evaluate both experimental models\n",
    "class_names = ['NORMAL', 'PNEUMONIA']\n",
    "\n",
    "xception_lstm_results = evaluate_experimental_model(\n",
    "    trained_xception_lstm, test_loader, class_names, \"Xception-LSTM\"\n",
    ")\n",
    "\n",
    "custom_lstm_results = evaluate_experimental_model(\n",
    "    trained_custom_lstm, test_loader, class_names, \"Custom CNN-LSTM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experimental Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare experimental models\n",
    "import pandas as pd\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': 'Xception-LSTM',\n",
    "        'Parameters': f\"{xception_lstm_history['parameter_count']:,}\",\n",
    "        'Final Accuracy (%)': f\"{xception_lstm_results['accuracy']:.2f}\",\n",
    "        'Best Test Acc (%)': f\"{max(xception_lstm_history['test_acc']):.2f}\",\n",
    "        'Avg Epoch Time (s)': f\"{np.mean(xception_lstm_history['epoch_times']):.1f}\",\n",
    "        'Architecture': 'Advanced (Xception + LSTM)'\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Custom CNN-LSTM',\n",
    "        'Parameters': f\"{custom_lstm_history['parameter_count']:,}\",\n",
    "        'Final Accuracy (%)': f\"{custom_lstm_results['accuracy']:.2f}\",\n",
    "        'Best Test Acc (%)': f\"{max(custom_lstm_history['test_acc']):.2f}\",\n",
    "        'Avg Epoch Time (s)': f\"{np.mean(custom_lstm_history['epoch_times']):.1f}\",\n",
    "        'Architecture': 'Lightweight (3-layer CNN + LSTM)'\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Experimental Model Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Plot training curves comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training loss comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(xception_lstm_history['train_loss'], label='Xception-LSTM', marker='o', linewidth=2)\n",
    "plt.plot(custom_lstm_history['train_loss'], label='Custom CNN-LSTM', marker='s', linewidth=2)\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(xception_lstm_history['train_acc'], label='Xception-LSTM', marker='o', linewidth=2)\n",
    "plt.plot(custom_lstm_history['train_acc'], label='Custom CNN-LSTM', marker='s', linewidth=2)\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(xception_lstm_history['test_acc'], label='Xception-LSTM', marker='o', linewidth=2)\n",
    "plt.plot(custom_lstm_history['test_acc'], label='Custom CNN-LSTM', marker='s', linewidth=2)\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance vs Complexity Analysis\n",
    "print(f\"\\nPerformance vs Complexity Analysis:\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "xception_params = xception_lstm_history['parameter_count']\n",
    "custom_params = custom_lstm_history['parameter_count']\n",
    "xception_acc = xception_lstm_results['accuracy']\n",
    "custom_acc = custom_lstm_results['accuracy']\n",
    "xception_time = np.mean(xception_lstm_history['epoch_times'])\n",
    "custom_time = np.mean(custom_lstm_history['epoch_times'])\n",
    "\n",
    "print(f\"Parameter Ratio: {xception_params / custom_params:.1f}x more parameters in Xception-LSTM\")\n",
    "print(f\"Accuracy Difference: {xception_acc - custom_acc:.2f}% accuracy gain\")\n",
    "print(f\"Training Time Ratio: {xception_time / custom_time:.1f}x longer training per epoch\")\n",
    "print(f\"Efficiency Score (Acc/Params): Xception={xception_acc/xception_params*1e6:.2f}, Custom={custom_acc/custom_params*1e6:.2f}\")\n",
    "\n",
    "# Medical significance analysis\n",
    "print(f\"\\nMedical Significance Analysis:\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "for result in [xception_lstm_results, custom_lstm_results]:\n",
    "    tn, fp, fn, tp = result['confusion_matrix'].ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{result['model_name']}:\")\n",
    "    print(f\"  Sensitivity (Pneumonia Detection): {sensitivity:.4f}\")\n",
    "    print(f\"  Specificity (Normal Identification): {specificity:.4f}\")\n",
    "    print(f\"  Missed Pneumonia Cases: {fn} (Critical metric)\")\n",
    "    print(f\"  False Alarms: {fp} (Secondary concern)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grad-CAM Visualization for CNN-LSTM Models\n",
    "\n",
    "**Visualization Challenge:**\n",
    "- Traditional Grad-CAM works on CNN layers\n",
    "- LSTM processing happens after spatial feature extraction\n",
    "- Need to visualize CNN features that influence LSTM decisions\n",
    "\n",
    "**Approach:**\n",
    "- Hook gradients from final CNN layer (before LSTM)\n",
    "- Show which spatial regions most influence final predictions\n",
    "- Compare attention patterns between different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_cnn_lstm(model, input_tensor, target_class=None, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Grad-CAM visualization for CNN-LSTM models.\n",
    "    \n",
    "    Shows which spatial regions of the CNN features most influence\n",
    "    the final LSTM-based prediction.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CNN-LSTM model\n",
    "        input_tensor: Input image tensor [1, 3, 224, 224]\n",
    "        target_class: Class to visualize (None for predicted class)\n",
    "        alpha: Overlay transparency\n",
    "        \n",
    "    Returns:\n",
    "        cam, heatmap, overlay for visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    gradients = []\n",
    "    activations = []\n",
    "    \n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "    \n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients.append(grad_output[0])\n",
    "    \n",
    "    # Determine which layer to hook based on model type\n",
    "    if hasattr(model, 'xception'):\n",
    "        # Xception-LSTM: hook the last Xception feature layer\n",
    "        target_layer = model.xception.body.conv4\n",
    "    else:\n",
    "        # Custom CNN-LSTM: hook the last conv layer\n",
    "        target_layer = model.cnn_backbone[6]  # Last conv layer\n",
    "    \n",
    "    # Register hooks\n",
    "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
    "    backward_handle = target_layer.register_full_backward_hook(backward_hook)\n",
    "    \n",
    "    # Disable cuDNN for LSTM backward compatibility\n",
    "    prev_cudnn = torch.backends.cudnn.enabled\n",
    "    if not was_training:\n",
    "        torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "        # Determine target class\n",
    "        if target_class is None:\n",
    "            prob = torch.sigmoid(output).item()\n",
    "            target_class = 1 if prob >= 0.5 else 0\n",
    "        \n",
    "        # Calculate score for target class\n",
    "        score = output[0, 0] if target_class == 1 else -output[0, 0]\n",
    "        \n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        score.backward()\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        torch.backends.cudnn.enabled = prev_cudnn\n",
    "        forward_handle.remove()\n",
    "        backward_handle.remove()\n",
    "        model.train(was_training)\n",
    "    \n",
    "    # Generate CAM\n",
    "    grads = gradients[0].detach().cpu().numpy()[0]\n",
    "    acts = activations[0].detach().cpu().numpy()[0]\n",
    "    \n",
    "    # Global average pooling of gradients\n",
    "    weights = grads.mean(axis=(1, 2))\n",
    "    \n",
    "    # Generate weighted feature map\n",
    "    cam = np.zeros(acts.shape[1:], dtype=np.float32)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * acts[i]\n",
    "    \n",
    "    # Apply ReLU and normalize\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "    \n",
    "    # Prepare visualization\n",
    "    img = input_tensor[0].detach().cpu().permute(1, 2, 0).numpy()\n",
    "    img = np.clip((img * 0.5) + 0.5, 0, 1)  # Denormalize\n",
    "    \n",
    "    # Resize CAM to image size\n",
    "    cam_resized = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap_bgr = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "    heatmap_rgb = cv2.cvtColor(heatmap_bgr, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Create overlay\n",
    "    overlay = alpha * heatmap_rgb + (1 - alpha) * img\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    return cam_resized, heatmap_rgb, overlay\n",
    "\n",
    "# Test Grad-CAM on both models\n",
    "print(\"Generating Grad-CAM visualizations for experimental models...\")\n",
    "\n",
    "# Get a sample image\n",
    "sample_images, sample_labels = next(iter(test_loader))\n",
    "sample_image = sample_images[5].unsqueeze(0).to(device)  # Pick one image\n",
    "\n",
    "# Generate Grad-CAM for both models\n",
    "print(\"\\nXception-LSTM Grad-CAM:\")\n",
    "xception_cam, xception_heatmap, xception_overlay = gradcam_cnn_lstm(\n",
    "    trained_xception_lstm, sample_image, target_class=1, alpha=0.5\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(sample_image[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(xception_cam, cmap='jet')\n",
    "plt.title('Xception-LSTM Grad-CAM')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(xception_overlay)\n",
    "plt.title('Xception-LSTM Overlay')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCustom CNN-LSTM Grad-CAM:\")\n",
    "custom_cam, custom_heatmap, custom_overlay = gradcam_cnn_lstm(\n",
    "    trained_custom_lstm, sample_image, target_class=1, alpha=0.5\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(sample_image[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(custom_cam, cmap='jet')\n",
    "plt.title('Custom CNN-LSTM Grad-CAM')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(custom_overlay)\n",
    "plt.title('Custom CNN-LSTM Overlay')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Grad-CAM visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Research Conclusions and Future Directions\n",
    "\n",
    "### Experimental Findings\n",
    "\n",
    "**Performance Analysis:**\n",
    "- **Xception-LSTM**: Advanced architecture with rich feature representation\n",
    "- **Custom CNN-LSTM**: Lightweight alternative with competitive performance\n",
    "- **Spatial Tokens**: 49-token approach provides spatial context for LSTM\n",
    "- **Sequential Processing**: LSTM adds contextual understanding to CNN features\n",
    "\n",
    "**Key Research Questions Answered:**\n",
    "1. **Do LSTMs improve CNN performance?** Results show [analyze based on actual results]\n",
    "2. **Is complex backbone necessary?** Comparison reveals trade-offs between complexity and accuracy\n",
    "3. **What do CNN-LSTM models focus on?** Grad-CAM shows attention patterns\n",
    "\n",
    "### Limitations and Challenges\n",
    "\n",
    "**Technical Limitations:**\n",
    "- **Computational Overhead**: LSTM processing adds significant computation\n",
    "- **Memory Requirements**: Storing spatial tokens increases memory usage\n",
    "- **Training Complexity**: More hyperparameters to tune\n",
    "- **Visualization Challenges**: Grad-CAM only shows CNN component, not LSTM attention\n",
    "\n",
    "**Research Limitations:**\n",
    "- **Limited Dataset**: Single pneumonia dataset may not generalize\n",
    "- **Spatial Token Order**: Current implementation uses raster order, other orders unexplored\n",
    "- **LSTM Alternatives**: Other sequence models (Transformers, GRU) not compared\n",
    "- **Medical Validation**: Requires validation by medical professionals\n",
    "\n",
    "### Future Research Directions\n",
    "\n",
    "**Technical Improvements:**\n",
    "1. **Attention Mechanisms**: Replace LSTM with self-attention for better spatial modeling\n",
    "2. **Token Order Optimization**: Investigate optimal spatial token ordering strategies\n",
    "3. **Multi-Scale Features**: Combine features from multiple CNN layers\n",
    "4. **Ensemble Integration**: Combine CNN-LSTM with other model types\n",
    "\n",
    "**Medical Applications:**\n",
    "1. **Multi-Class Extension**: Extend to multiple lung conditions\n",
    "2. **Severity Assessment**: Predict pneumonia severity levels\n",
    "3. **Cross-Dataset Validation**: Test on different hospital datasets\n",
    "4. **Clinical Integration**: Develop deployment-ready systems\n",
    "\n",
    "**Research Extensions:**\n",
    "1. **Comparative Studies**: Compare with other hybrid architectures\n",
    "2. **Ablation Studies**: Analyze contribution of each component\n",
    "3. **Efficiency Optimization**: Develop mobile-friendly versions\n",
    "4. **Interpretability**: Better visualization of LSTM decision process\n",
    "\n",
    "### Impact and Significance\n",
    "\n",
    "**Scientific Contribution:**\n",
    "- Demonstrated feasibility of CNN-LSTM for medical imaging\n",
    "- Established baseline performance for spatial token processing\n",
    "- Provided comparison framework for hybrid architectures\n",
    "- Highlighted trade-offs between complexity and performance\n",
    "\n",
    "**Clinical Relevance:**\n",
    "- Potential for improved pneumonia detection accuracy\n",
    "- Framework for incorporating spatial context in medical AI\n",
    "- Foundation for multi-modal medical image analysis\n",
    "- Step toward more interpretable medical AI systems\n",
    "\n",
    "**This experimental work opens new avenues for combining CNN and sequential processing in medical image analysis, with promising results for pediatric pneumonia detection.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}