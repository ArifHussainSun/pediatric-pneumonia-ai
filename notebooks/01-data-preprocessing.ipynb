{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Augmentation for Pediatric Pneumonia Detection\n",
    "\n",
    "## Overview\n",
    "This notebook handles all data preprocessing tasks including:\n",
    "- **Image augmentation**: Creating variations of X-ray images to improve model training\n",
    "- **Data splitting**: Organizing data into training and testing sets\n",
    "- **Feature extraction**: Extracting statistical features from X-ray images for traditional ML models\n",
    "\n",
    "## Key Concepts\n",
    "- **Augmentation**: Artificially increasing dataset size by applying transformations (rotation, flip, etc.)\n",
    "- **ROI (Region of Interest)**: Dividing images into smaller sections to analyze specific areas\n",
    "- **Feature extraction**: Converting images into numerical data that traditional ML algorithms can use\n",
    "- **Denoising**: Removing noise from medical images to improve quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np  # For numerical operations\n",
    "import random  # For random transformations\n",
    "import os  # For file operations\n",
    "import pandas as pd  # For data handling\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "from pathlib import Path  # For file path operations\n",
    "import shutil  # For file copying\n",
    "from scipy import stats  # For statistical calculations\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Augmentation Functions\n",
    "\n",
    "**What is Image Augmentation?**\n",
    "- Creates new training images by applying small changes to existing ones\n",
    "- Helps models learn to recognize pneumonia in different conditions\n",
    "- Prevents overfitting (memorizing specific images instead of learning patterns)\n",
    "\n",
    "**Techniques Used:**\n",
    "- **Denoising**: Removes image noise common in X-rays\n",
    "- **Horizontal flip**: Mirror image left-to-right (lungs are symmetric)\n",
    "- **Shear**: Slight stretching/skewing of the image\n",
    "- **Rotation**: Small rotations to simulate different patient positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image_efficient(image):\n",
    "    \"\"\"\n",
    "    Apply efficient random augmentation techniques to a chest X-ray image.\n",
    "    \n",
    "    This function improves image quality and creates variations for training.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): Input grayscale X-ray image\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Processed and augmented image\n",
    "    \"\"\"\n",
    "    # Ensure image is in the right format\n",
    "    if len(image.shape) == 3 and image.shape[2] == 1:\n",
    "        image = image.squeeze()  # Remove extra dimension if present\n",
    "    h, w = image.shape\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # Step 1: Remove noise from X-ray (denoising)\n",
    "    # Medical X-rays often have noise that can confuse models\n",
    "    # h=10: strength of denoising, templateWindowSize=7: comparison window\n",
    "    denoised_image = cv2.fastNlMeansDenoising(image, None, h=10, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "    # Step 2: Random horizontal flip (50% chance)\n",
    "    # Since lungs are symmetric, flipping doesn't change medical meaning\n",
    "    if random.getrandbits(1):  # Faster than random.random() > 0.5\n",
    "        cv2.flip(denoised_image, 1, denoised_image)  # Flip horizontally in-place\n",
    "\n",
    "    # Step 3: Apply random shear (stretching) transformation\n",
    "    # Simulates slight variations in X-ray positioning\n",
    "    vertical_shear = np.tan(np.radians(random.uniform(0, 10)))  # 0-10 degrees\n",
    "    horizontal_shear = np.tan(np.radians(random.uniform(0, 10)))\n",
    "    \n",
    "    # Create transformation matrix for shearing\n",
    "    shear_matrix = np.array([\n",
    "        [1, horizontal_shear, 0],\n",
    "        [vertical_shear, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # Apply shear transformation\n",
    "    denoised_image = cv2.warpPerspective(denoised_image, shear_matrix, (w, h),\n",
    "                                       flags=cv2.INTER_LINEAR,\n",
    "                                       borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # Step 4: Random rotation (-3 to +3 degrees)\n",
    "    # Small rotations simulate different patient positioning\n",
    "    rotation_angle = random.uniform(-3, 3)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((w/2, h/2), rotation_angle, 1.0)\n",
    "    denoised_image = cv2.warpAffine(denoised_image, rotation_matrix, (w, h),\n",
    "                                  flags=cv2.INTER_LINEAR,\n",
    "                                  borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    return denoised_image\n",
    "\n",
    "# Test the augmentation function\n",
    "print(\"Image augmentation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistical Feature Extraction\n",
    "\n",
    "**Why Extract Features?**\n",
    "- Traditional ML algorithms (like SVM) need numerical features, not raw images\n",
    "- Statistical features capture important image properties\n",
    "- Can be combined with deep learning for ensemble methods\n",
    "\n",
    "**Features Extracted:**\n",
    "- **Basic stats**: Mean, median, min, max (brightness levels)\n",
    "- **Distribution**: Skewness (asymmetry), kurtosis (tail heaviness)\n",
    "- **Percentiles**: Values at specific percentage points\n",
    "- **Energy**: Sum of squared pixel values\n",
    "- **Entropy**: Measure of randomness/texture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Extract statistical features from a grayscale chest X-ray image.\n",
    "    \n",
    "    These features capture important statistical properties that can help\n",
    "    distinguish between normal and pneumonia X-rays.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): Grayscale image as a 2D NumPy array\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing statistical features\n",
    "    \"\"\"\n",
    "    # Convert image to 1D array for statistical calculations\n",
    "    pixels = image.flatten().astype(float)\n",
    "    \n",
    "    # Initialize dictionary to store features\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistical measures\n",
    "    features['maximum'] = np.max(pixels)  # Brightest pixel\n",
    "    features['minimum'] = np.min(pixels)  # Darkest pixel\n",
    "    features['mean'] = np.mean(pixels)    # Average brightness\n",
    "    features['median'] = np.median(pixels) # Middle value when sorted\n",
    "    features['std_dev'] = np.std(pixels)  # Spread of pixel values\n",
    "    \n",
    "    # Mode (most common pixel value)\n",
    "    features['mode'] = stats.mode(pixels, keepdims=True)[0][0]\n",
    "    \n",
    "    # Distribution shape measures\n",
    "    if np.std(pixels) < 1e-10:  # Handle uniform images\n",
    "        features['skewness'] = 0.0\n",
    "        features['kurtosis'] = 0.0\n",
    "    else:\n",
    "        # Skewness: asymmetry of distribution (+ = tail on right, - = tail on left)\n",
    "        skew_val = stats.skew(pixels)\n",
    "        features['skewness'] = 0.0 if np.isnan(skew_val) else skew_val\n",
    "        \n",
    "        # Kurtosis: \"tailedness\" of distribution (higher = more extreme values)\n",
    "        kurt_val = stats.kurtosis(pixels)\n",
    "        features['kurtosis'] = 0.0 if np.isnan(kurt_val) else kurt_val\n",
    "    \n",
    "    # Percentiles (values at specific percentage points)\n",
    "    features['quantile_2.5'] = np.percentile(pixels, 2.5)   # Very dark pixels\n",
    "    features['quantile_5'] = np.percentile(pixels, 5)       # Dark pixels\n",
    "    features['quantile_10'] = np.percentile(pixels, 10)     # Darker pixels\n",
    "    features['quantile_90'] = np.percentile(pixels, 90)     # Brighter pixels\n",
    "    features['quantile_95'] = np.percentile(pixels, 95)     # Bright pixels\n",
    "    features['quantile_97.5'] = np.percentile(pixels, 97.5) # Very bright pixels\n",
    "    \n",
    "    # Energy measure (sum of squared pixel values)\n",
    "    features['absolute_energy'] = np.sum(pixels ** 2)\n",
    "    \n",
    "    # Entropy (measure of randomness/texture)\n",
    "    hist, _ = np.histogram(pixels, bins=256, range=(0, 255), density=True)\n",
    "    hist = hist[hist > 0]  # Remove zeros to avoid log(0)\n",
    "    features['entropy'] = -np.sum(hist * np.log2(hist))\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROI (Region of Interest) Feature Extraction\n",
    "\n",
    "**What is ROI Analysis?**\n",
    "- Divides X-ray image into a grid (e.g., 4x4 = 16 regions)\n",
    "- Extracts features from each region separately\n",
    "- Helps identify which parts of lungs show pneumonia signs\n",
    "- Different grid sizes capture different levels of detail\n",
    "\n",
    "**Grid Sizes Used:**\n",
    "- **1x1**: Whole image (global features)\n",
    "- **4x4**: 16 regions (moderate detail)\n",
    "- **16x16**: 256 regions (fine detail)\n",
    "\n",
    "**Benefits:**\n",
    "- Pneumonia often affects specific lung regions\n",
    "- Provides spatial information about disease location\n",
    "- More features can improve traditional ML performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roi_features(image, grid_size=(4, 4)):\n",
    "    \"\"\"\n",
    "    Extract statistical features from each ROI (Region of Interest) in a grid.\n",
    "    \n",
    "    This function divides the X-ray into smaller regions and extracts features\n",
    "    from each region separately, providing spatial information.\n",
    "    \n",
    "    Parameters:\n",
    "    image (numpy.ndarray): Grayscale image as a 2D NumPy array\n",
    "    grid_size (tuple): Grid dimensions (rows, cols) for ROI division\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing statistical features for each ROI\n",
    "    \"\"\"\n",
    "    height, width = image.shape\n",
    "    rows, cols = grid_size\n",
    "    \n",
    "    # Calculate size of each ROI\n",
    "    roi_height = height // rows\n",
    "    roi_width = width // cols\n",
    "    \n",
    "    # Store all ROI pixel data first (more efficient)\n",
    "    roi_pixels_list = []\n",
    "    \n",
    "    # Extract all ROIs\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            # Calculate ROI boundaries\n",
    "            start_row = row * roi_height\n",
    "            end_row = start_row + roi_height\n",
    "            start_col = col * roi_width\n",
    "            end_col = start_col + roi_width\n",
    "            \n",
    "            # Extract ROI and flatten to 1D\n",
    "            roi = image[start_row:end_row, start_col:end_col]\n",
    "            roi_pixels = roi.flatten().astype(np.float32)\n",
    "            roi_pixels_list.append(roi_pixels)\n",
    "    \n",
    "    # Extract features from each ROI\n",
    "    all_features = {}\n",
    "    \n",
    "    # Percentiles to calculate for efficiency\n",
    "    percentiles = [2.5, 5, 10, 50, 90, 95, 97.5]  # Include median (50th)\n",
    "    \n",
    "    for roi_idx, pixels in enumerate(roi_pixels_list):\n",
    "        roi_num = roi_idx + 1  # ROI numbering starts from 1\n",
    "        \n",
    "        # Basic statistics\n",
    "        features = {\n",
    "            f'maximum_roi_{roi_num}': np.max(pixels),\n",
    "            f'minimum_roi_{roi_num}': np.min(pixels),\n",
    "            f'mean_roi_{roi_num}': np.mean(pixels),\n",
    "            f'std_dev_roi_{roi_num}': np.std(pixels)\n",
    "        }\n",
    "        \n",
    "        # Calculate all percentiles at once (more efficient)\n",
    "        perc_values = np.percentile(pixels, percentiles)\n",
    "        features[f'quantile_2.5_roi_{roi_num}'] = perc_values[0]\n",
    "        features[f'quantile_5_roi_{roi_num}'] = perc_values[1]\n",
    "        features[f'quantile_10_roi_{roi_num}'] = perc_values[2]\n",
    "        features[f'median_roi_{roi_num}'] = perc_values[3]\n",
    "        features[f'quantile_90_roi_{roi_num}'] = perc_values[4]\n",
    "        features[f'quantile_95_roi_{roi_num}'] = perc_values[5]\n",
    "        features[f'quantile_97.5_roi_{roi_num}'] = perc_values[6]\n",
    "        \n",
    "        # Energy calculation\n",
    "        features[f'absolute_energy_roi_{roi_num}'] = np.sum(pixels * pixels)\n",
    "        \n",
    "        # Skip expensive calculations for uniform regions\n",
    "        std_val = features[f'std_dev_roi_{roi_num}']\n",
    "        if std_val < 1e-10:  # Nearly uniform region\n",
    "            features[f'mode_roi_{roi_num}'] = features[f'mean_roi_{roi_num}']\n",
    "            features[f'skewness_roi_{roi_num}'] = 0.0\n",
    "            features[f'kurtosis_roi_{roi_num}'] = 0.0\n",
    "            features[f'entropy_roi_{roi_num}'] = 0.0\n",
    "        else:\n",
    "            # Mode calculation\n",
    "            try:\n",
    "                mode_result = stats.mode(pixels, keepdims=True)\n",
    "                features[f'mode_roi_{roi_num}'] = mode_result[0][0]\n",
    "            except:\n",
    "                features[f'mode_roi_{roi_num}'] = features[f'mean_roi_{roi_num}']\n",
    "            \n",
    "            # Distribution shape\n",
    "            skew_val = stats.skew(pixels)\n",
    "            features[f'skewness_roi_{roi_num}'] = 0.0 if np.isnan(skew_val) else skew_val\n",
    "            \n",
    "            kurt_val = stats.kurtosis(pixels)\n",
    "            features[f'kurtosis_roi_{roi_num}'] = 0.0 if np.isnan(kurt_val) else kurt_val\n",
    "            \n",
    "            # Entropy (only for reasonably sized ROIs)\n",
    "            if len(pixels) > 100:\n",
    "                hist, _ = np.histogram(pixels, bins=128, range=(0, 255), density=True)\n",
    "                hist = hist[hist > 1e-10]\n",
    "                if len(hist) > 0:\n",
    "                    features[f'entropy_roi_{roi_num}'] = -np.sum(hist * np.log2(hist))\n",
    "                else:\n",
    "                    features[f'entropy_roi_{roi_num}'] = 0.0\n",
    "            else:\n",
    "                features[f'entropy_roi_{roi_num}'] = 0.0\n",
    "        \n",
    "        all_features.update(features)\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "print(\"ROI feature extraction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting Utilities\n",
    "\n",
    "**Why Split Data Properly?**\n",
    "- Need separate training and testing sets\n",
    "- Training set: Used to teach the model\n",
    "- Testing set: Used to evaluate model performance (never seen during training)\n",
    "- Prevents overfitting by testing on unseen data\n",
    "\n",
    "**Splitting Strategy:**\n",
    "- Maintains class balance (equal normal/pneumonia ratios)\n",
    "- Typically 80% training, 20% testing\n",
    "- Random shuffle ensures fair distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSplitter:\n",
    "    \"\"\"\n",
    "    A utility class for splitting chest X-ray data into training and testing sets.\n",
    "    \n",
    "    This ensures proper data organization for machine learning training while\n",
    "    maintaining balanced class distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\".\", train_ratio=0.8):\n",
    "        \"\"\"\n",
    "        Initialize the data splitter.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Directory containing train and test folders\n",
    "            train_ratio: Fraction of data for training (0.8 = 80% train, 20% test)\n",
    "        \"\"\"\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.train_ratio = train_ratio\n",
    "        self.test_ratio = 1 - train_ratio\n",
    "        \n",
    "        # Define directory paths\n",
    "        self.original_train_dir = self.base_dir / \"train\"\n",
    "        self.original_test_dir = self.base_dir / \"test\"\n",
    "        self.new_train_dir = self.base_dir / \"new_train\"\n",
    "        self.new_test_dir = self.base_dir / \"new_test\"\n",
    "        \n",
    "        # Medical image classes\n",
    "        self.classes = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "    \n",
    "    def aggregate_files(self):\n",
    "        \"\"\"Collect all image files from both train and test directories.\"\"\"\n",
    "        aggregated = {\"NORMAL\": [], \"PNEUMONIA\": []}\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            # Collect from training directory\n",
    "            train_class_dir = self.original_train_dir / class_name\n",
    "            if train_class_dir.exists():\n",
    "                # Find all common image formats\n",
    "                files = (list(train_class_dir.glob(\"*.jpeg\")) + \n",
    "                        list(train_class_dir.glob(\"*.jpg\")) + \n",
    "                        list(train_class_dir.glob(\"*.png\")))\n",
    "                aggregated[class_name].extend([(f, \"train\") for f in files])\n",
    "            \n",
    "            # Collect from test directory\n",
    "            test_class_dir = self.original_test_dir / class_name\n",
    "            if test_class_dir.exists():\n",
    "                files = (list(test_class_dir.glob(\"*.jpeg\")) + \n",
    "                        list(test_class_dir.glob(\"*.jpg\")) + \n",
    "                        list(test_class_dir.glob(\"*.png\")))\n",
    "                aggregated[class_name].extend([(f, \"test\") for f in files])\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def calculate_split_counts(self, pneumonia_count, normal_count):\n",
    "        \"\"\"\n",
    "        Calculate how many files should go to train and test.\n",
    "        \n",
    "        Uses pneumonia count as reference to maintain class balance.\n",
    "        \n",
    "        Args:\n",
    "            pneumonia_count: Total number of pneumonia images\n",
    "            normal_count: Total number of normal images\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with split counts for each class\n",
    "        \"\"\"\n",
    "        # Calculate target counts based on pneumonia distribution\n",
    "        target_pneumonia_test = int(pneumonia_count * self.test_ratio)\n",
    "        target_pneumonia_train = pneumonia_count - target_pneumonia_test\n",
    "        \n",
    "        # Match normal test count to pneumonia for balanced testing\n",
    "        target_normal_test = target_pneumonia_test\n",
    "        target_normal_train = normal_count - target_normal_test\n",
    "        \n",
    "        return {\n",
    "            \"pneumonia\": {\"train\": target_pneumonia_train, \"test\": target_pneumonia_test},\n",
    "            \"normal\": {\"train\": target_normal_train, \"test\": target_normal_test}\n",
    "        }\n",
    "    \n",
    "    def create_directories(self):\n",
    "        \"\"\"Create new directory structure for split data.\"\"\"\n",
    "        # Remove existing directories if they exist\n",
    "        for dir_path in [self.new_train_dir, self.new_test_dir]:\n",
    "            if dir_path.exists():\n",
    "                shutil.rmtree(dir_path)\n",
    "        \n",
    "        # Create new directory structure\n",
    "        for split in [\"new_train\", \"new_test\"]:\n",
    "            for class_name in self.classes:\n",
    "                dir_path = self.base_dir / split / class_name\n",
    "                dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def split_data(self):\n",
    "        \"\"\"Main method to split the data according to the specified ratio.\"\"\"\n",
    "        print(\"Aggregating files...\")\n",
    "        aggregated = self.aggregate_files()\n",
    "        \n",
    "        pneumonia_files = aggregated[\"PNEUMONIA\"]\n",
    "        normal_files = aggregated[\"NORMAL\"]\n",
    "        \n",
    "        print(f\"Total PNEUMONIA files: {len(pneumonia_files)}\")\n",
    "        print(f\"Total NORMAL files: {len(normal_files)}\")\n",
    "        \n",
    "        # Calculate split counts\n",
    "        split_counts = self.calculate_split_counts(len(pneumonia_files), len(normal_files))\n",
    "        \n",
    "        print(\"\\nTarget split counts:\")\n",
    "        print(f\"PNEUMONIA - Train: {split_counts['pneumonia']['train']}, Test: {split_counts['pneumonia']['test']}\")\n",
    "        print(f\"NORMAL - Train: {split_counts['normal']['train']}, Test: {split_counts['normal']['test']}\")\n",
    "        \n",
    "        # Create directories and copy files\n",
    "        self.create_directories()\n",
    "        \n",
    "        # Shuffle files for random distribution\n",
    "        random.shuffle(pneumonia_files)\n",
    "        random.shuffle(normal_files)\n",
    "        \n",
    "        # Split and copy files\n",
    "        self._copy_split_files(pneumonia_files, normal_files, split_counts)\n",
    "        \n",
    "        print(\"\\nData split completed successfully!\")\n",
    "        return split_counts\n",
    "    \n",
    "    def _copy_split_files(self, pneumonia_files, normal_files, split_counts):\n",
    "        \"\"\"Copy files to their designated train/test directories.\"\"\"\n",
    "        print(\"\\nCopying files...\")\n",
    "        \n",
    "        # Split pneumonia files\n",
    "        pneumonia_train = pneumonia_files[:split_counts[\"pneumonia\"][\"train\"]]\n",
    "        pneumonia_test = pneumonia_files[split_counts[\"pneumonia\"][\"train\"]:]\n",
    "        \n",
    "        # Split normal files\n",
    "        normal_test = normal_files[:split_counts[\"normal\"][\"test\"]]\n",
    "        normal_train = normal_files[split_counts[\"normal\"][\"test\"]:]\n",
    "        \n",
    "        # Copy files to appropriate directories\n",
    "        for file_path, _ in pneumonia_train:\n",
    "            dest = self.new_train_dir / \"PNEUMONIA\" / file_path.name\n",
    "            shutil.copy2(file_path, dest)\n",
    "        \n",
    "        for file_path, _ in pneumonia_test:\n",
    "            dest = self.new_test_dir / \"PNEUMONIA\" / file_path.name\n",
    "            shutil.copy2(file_path, dest)\n",
    "        \n",
    "        for file_path, _ in normal_train:\n",
    "            dest = self.new_train_dir / \"NORMAL\" / file_path.name\n",
    "            shutil.copy2(file_path, dest)\n",
    "        \n",
    "        for file_path, _ in normal_test:\n",
    "            dest = self.new_test_dir / \"NORMAL\" / file_path.name\n",
    "            shutil.copy2(file_path, dest)\n",
    "\n",
    "print(\"Data splitting utility defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Feature Extraction for Machine Learning\n",
    "\n",
    "**Purpose:**\n",
    "- Process entire datasets to create feature files\n",
    "- Creates CSV files with features for each image\n",
    "- Used by traditional ML algorithms (SVM, Random Forest)\n",
    "- Different ROI schemes provide different levels of spatial detail\n",
    "\n",
    "**Output:**\n",
    "- CSV files with rows = images, columns = features\n",
    "- Includes image ID and label (NORMAL=1, PNEUMONIA=0)\n",
    "- Can be loaded directly into scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(input_dir, output_file, roi_scheme=(16, 16)):\n",
    "    \"\"\"\n",
    "    Extract features from all images in a dataset and save to CSV.\n",
    "    \n",
    "    This function processes entire directories of X-ray images and creates\n",
    "    feature files suitable for traditional machine learning algorithms.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dir (str): Directory containing NORMAL and PNEUMONIA subdirectories\n",
    "    output_file (str): Path for output CSV file\n",
    "    roi_scheme (tuple): Grid size for ROI analysis (rows, cols)\n",
    "    \"\"\"\n",
    "    source_dir = os.path.join(os.getcwd(), input_dir)\n",
    "    normal_dir = os.path.join(source_dir, 'NORMAL')\n",
    "    pneumonia_dir = os.path.join(source_dir, 'PNEUMONIA')\n",
    "    \n",
    "    data = []\n",
    "    count = 0\n",
    "    \n",
    "    print(f\"Extracting features using {roi_scheme[0]}x{roi_scheme[1]} ROI scheme...\")\n",
    "    \n",
    "    # Process NORMAL images\n",
    "    print(\"Processing NORMAL images...\")\n",
    "    if os.path.exists(normal_dir):\n",
    "        normal_files = os.listdir(normal_dir)\n",
    "        for filename in tqdm(normal_files, desc=\"NORMAL\"):\n",
    "            if count % 100 == 0 and count > 0:\n",
    "                print(f\"Processed {count} images...\")\n",
    "            \n",
    "            path = os.path.join(normal_dir, filename)\n",
    "            # Load image in grayscale\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract ROI features\n",
    "            features = extract_roi_features(img, grid_size=roi_scheme)\n",
    "            features['image_id'] = filename\n",
    "            features['label'] = 1  # NORMAL = 1\n",
    "            data.append(features)\n",
    "            count += 1\n",
    "    \n",
    "    # Process PNEUMONIA images\n",
    "    print(\"Processing PNEUMONIA images...\")\n",
    "    if os.path.exists(pneumonia_dir):\n",
    "        pneumonia_files = os.listdir(pneumonia_dir)\n",
    "        for filename in tqdm(pneumonia_files, desc=\"PNEUMONIA\"):\n",
    "            if count % 100 == 0 and count > 0:\n",
    "                print(f\"Processed {count} images...\")\n",
    "            \n",
    "            path = os.path.join(pneumonia_dir, filename)\n",
    "            # Load image in grayscale\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract ROI features\n",
    "            features = extract_roi_features(img, grid_size=roi_scheme)\n",
    "            features['image_id'] = filename\n",
    "            features['label'] = 0  # PNEUMONIA = 0\n",
    "            data.append(features)\n",
    "            count += 1\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nFeature extraction completed!\")\n",
    "    print(f\"Total images processed: {count}\")\n",
    "    print(f\"Features per image: {len(df.columns) - 2}\")  # Exclude image_id and label\n",
    "    print(f\"Output saved to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Batch feature extraction function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Functions\n",
    "\n",
    "Let's test our preprocessing functions with a sample image to make sure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image from our dataset\n",
    "sample_dir = \"../data/training/augmented_train/NORMAL\"\n",
    "if os.path.exists(sample_dir):\n",
    "    # Get the first image file\n",
    "    image_files = [f for f in os.listdir(sample_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if image_files:\n",
    "        sample_path = os.path.join(sample_dir, image_files[0])\n",
    "        \n",
    "        # Load and test image processing\n",
    "        img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
    "        print(f\"Loaded sample image: {sample_path}\")\n",
    "        print(f\"Image shape: {img.shape}\")\n",
    "        \n",
    "        # Test augmentation\n",
    "        augmented_img = augment_image_efficient(img)\n",
    "        print(f\"Augmentation successful! Output shape: {augmented_img.shape}\")\n",
    "        \n",
    "        # Test feature extraction\n",
    "        features = extract_features(img)\n",
    "        print(f\"\\nExtracted {len(features)} global features\")\n",
    "        print(\"Sample features:\", list(features.keys())[:5])\n",
    "        \n",
    "        # Test ROI feature extraction\n",
    "        roi_features = extract_roi_features(img, grid_size=(4, 4))\n",
    "        print(f\"\\nExtracted {len(roi_features)} ROI features (4x4 grid)\")\n",
    "        print(\"Sample ROI features:\", list(roi_features.keys())[:5])\n",
    "        \n",
    "        # Visualize original vs augmented\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(augmented_img, cmap='gray')\n",
    "        plt.title('Augmented Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(np.abs(img.astype(float) - augmented_img.astype(float)), cmap='hot')\n",
    "        plt.title('Difference (Hot = More Change)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nAll preprocessing functions working correctly!\")\n",
    "    else:\n",
    "        print(\"No image files found in the sample directory\")\n",
    "else:\n",
    "    print(f\"Sample directory not found: {sample_dir}\")\n",
    "    print(\"Make sure the dataset is properly copied to ../data/training/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook provides all the data preprocessing tools needed for the pneumonia detection project:\n",
    "\n",
    "### Completed Functions:\n",
    "1. **Image Augmentation**: Improves X-ray quality and creates training variations\n",
    "2. **Feature Extraction**: Converts images to numerical features for traditional ML\n",
    "3. **ROI Analysis**: Spatial feature extraction from image regions\n",
    "4. **Data Splitting**: Proper train/test separation with class balance\n",
    "5. **Batch Processing**: Automated feature extraction for entire datasets\n",
    "\n",
    "### Next Steps:\n",
    "- Use these functions in model training notebooks\n",
    "- Create feature CSV files for SVM and ensemble methods\n",
    "- Apply augmentation during deep learning training\n",
    "\n",
    "### Key Outputs:\n",
    "- Augmented images for training\n",
    "- Feature CSV files for traditional ML\n",
    "- Properly split train/test datasets\n",
    "\n",
    "**All preprocessing tools are now ready for use in subsequent notebooks!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}